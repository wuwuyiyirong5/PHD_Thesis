%%---------------------------------------------------------------------------------------------------%%
%%------ 第三章：基于4P1-P1元求解Navier-Stokes方程的移动网格方法 ------------------------------------%%
%%---------------------------------------------------------------------------------------------------%%


\chapter{基于4P1-P1元求解Navier-Stokes方程的移动网格方法}
\label{chapter:4P1-P1_moving}

    根据上一章, 我们知道混合元是求解不可压Navier-Stokes方程的一种重要
    方法。为了保证解的存在唯一性，需要使速度和压力两个解所在的空间满足
    LBB条件。其中一种办法是使得速度空间相对压力空间来说，自由度足够多，
    例如mini元，Taylor-Hood元等等。另外一中方法是对压力空间施加一些约束，
    比如说稳定化的$P_1-P_1$元，$P_1-P_0$元。为了减少计算量，提高计算效率，通常会
    运用自适应网格方法。在文献\cite{danaila2014newton},\cite{ebeida2009unsteady}
    \cite{berrone2009space}中，运用了h-自适应的P2P1元。在实际的工程计
    算中，我们通常倾向于使用线性元，而非高次元。
    \cite{zheng2010posteriori}提出自适应网格和稳定化P1P1元、P1P0元相
    结合的策略. \cite{di2005moving}将移动网格方法应用到不可压Navier-Stokes
    方程的求解。我们移动网格部分的策略是基于文献\cite{di2005moving}中
    的工作。然而，不满足LBB条件的有限元对应用到自适应网格方法上有一定的技术困难。
    综合以上考虑，我们选取稳定的$P_1isoP_2P_1$元，它自然满足LBB条件，详细见
    \cite{bercovier1979error}. 在P1ISOP2P1元，速度单元所在的网格可以有压
    单元的网格加密一次得到，如Figure (\ref{fig::p-v})注意到在速度单元
    $\vec{u}$ 和压力单元$p$ 上均是线性元。但是，速度单元网格上的6个基函数不
    在同一个速度单元上。因此在拼装散度块矩阵，即拼装压力单元和速度
    单元，这个过程并不显然。

    在本文的工作中，我们选取$4P1-P1$元，它与$P_1isoP_2P_1$拥有相同的
    网格结构，自然也满足inf-sup条件，但需要指出的是$P_1isoP_2P_1$元的
    速度单元上的基函数都局部的位于相同的速度单元内。这给我们拼装散度
    块矩阵提供了便利：只要我们建立四个速度单元和大的压力单元之间的索引。
    索引的建立又依赖于两层网格的数据结构，这种两层网格用我们上一章提到
    的几何遗传树结构，可以建立两层网格间的对应关系。

    基于这种网格遗传树的结构，我们可以很容易的建立四个小的速度单元与
    压力宏单元间的索引。同时在\cite{li2005multi}中h-自适应方法也是基
    于这种树结构。自然的，我们可以将h-自适应方法应用到4P1-P1元上。自
    适应网格可以减少计算量，同时可以研究流体局部小尺度上的现象，比如
    涡。同时，移动网格网格方法也可以应用在4P1-P1元上，网格移动我们只
    移动压力网格，当压力网格移动完，速度网格由压力网格加密一次即可得
    到。但是需要注意的是，求解不可压的Navier-Stokes方程，在将旧网格的
    解插值到移动后的网格的过程，要满足散度为0的条件，这个工作由
    \cite{di2005moving}中给出。

    在第一节中，我们介绍Navier-Stokes方程的知识。
    在第二节中我们展示4P1-P1的数据结构，以及单元间索引的建立过程，接下来
    我们用4P1-P1元近似Navier-Stokes方程。移动网格的策略将在第四节中给出。
    最后，我们给出数值例子。

\section{数据结构}
   4P1-P1是基于两套不同的网格和两种有限元空间。速度网格可以由压力网格
   全局加密一次得到。网格的数据结构是基于\cite{li2005multi}中的几何遗
   传树结构，如Figure(\ref{fig::hgrometrytree})所示。一个宏压力单元对
   应这四个速度单元，通过遍历一次所有的速度单元，利用几何遗传树结构，
   就可以建立速度单元和压力单元的$1-1$对应。

   在AFEPack中，速度网格和压力网格分别存储在两张非正则网格irregularMeshV
   和irregularMeshP上，这两张非正则网格都是建立在同一颗树上。非正则网格上，
   可以进行全局加密，局部加密以及疏化的操作，能进行这种操作得益于它的四叉树结构。
   所有的网格节点从根节点到叶子节点，都存储在非正则网格中。这里irregularMeshV
   是由irregularMeshP进行全局一次加密得到的，根据非正则网格的树结构，我们可以
   通过遍历irregularMeshV的全部活动单元(即叶子节点)，通过活动单元来找到它的
   父亲单元(即叶子节点的父亲节点)。这样我们建立了从速度网格单元到压力网格单元
   之间的单向索引。还是根据活动单元的父亲节点也是具有树结构的，因此，我们可以
   根据父子节点单元(压力单元)来找到它对应的四个儿子单元(即四个速度单元)，这样
   我们也建立了从压力单元到速度单元的单向索引。到这里，速度和压力间的单元已经建立了
   $1-1$索引，注意到，只要不产生新的网格单元，我们建立的索引不需要重新改。应用到
   移动网格上，我们只需要一次构建索引就可以，不管网格如何移动，网格间的索引不会改变。
   建立索引的过程参见算法(\ref{alg::index})。


   \begin{algorithm}
     \caption{构建速度单元和压力单元间的索引}
     \begin{algorithmic}[1]
       \State \text{achieveiterator} $\gets$ \text{irregualerMeshV.beginActiveElement()}
       \State \text{enditerator} $\gets$ \text{irregualerMeshV.endActiveElement()}

       \While {\text{achieveiterator} $\neq$ \text{enditerator}}
           \State \text{int index-v-element} $\gets$
                   \text{achieveiterator}$-\rangle$ \text{index}
           \State \text{HElement} $\langle$ \text{DIM,
                   DIM}$\rangle \ast$ \text{parent} $\gets$
                   \text{activeiterator}$-\rangle$ \text{parent}
           \State \text{int index-p-element} $\gets$
                   \text {parent}$-\rangle$ \text{index}
           \State \text{int n-child} $\gets$
                   \text{parent}$-\rangle$ \text{n-child}
           \State
                  \text{index-p2v[index-p-element].resize}(\text{n-child})
           \While {$i \geq 0$ \textbf{and} $i < \text{n-child}$}
                  \State \text{HElement}$\langle$\text{DIM,DIM}$\rangle$ $\ast$\text{chi}
                         $\gets$ \text{parent}$-\rangle$\text{child[i]}
                  \State \text{int index-v-element} $\gets$
                         \text{child}$-\rangle$\text{index}
                  \State \text{index-p2v[index-p-element][i]} $\gets$
                         \text{index-v-element}
                  \State \text{index-v2p[index-v-element]} $\gets$ \text{index-p-element}
           \EndWhile
       \EndWhile
     \end{algorithmic}
     \label{alg::index}
   \end{algorithm}

\section{混合元近似}
    \subsection{流体方程}
        在Stokes方程
        \begin{equation}
            \begin{aligned}
                -\bigtriangleup \vec{u} + \nabla p = f \quad \text{在}\Omega \text{内}, \\
                \nabla \cdot \vec{u} = 0 \quad \text{在}\Omega \text{内},\\
                \vec{u} = 0 \qquad \text{在}\partial \Omega \text{上},
            \end{aligned}
            \label{eq::stokes}
        \end{equation}
        的基础上加一个对流项，再加上时间项，我们可以得到一个时间发展的Navier-Stokes方程
        \begin{equation}
            \begin{aligned}
                \frac{\partial \vec{u}}{\partial t}-\nu \nabla^2\vec{u} + \vec{u}\cdot \nabla \vec{u} + \nabla p &=& \vec{f}, \\
                \nabla \cdot \vec{u} = 0.
            \end{aligned}
            \label{eq::NS_system}
        \end{equation}
        其中$\nu > 0$ 是一个常数，称作动力学粘性系数。跟Stokes 方程中类似，$\vec{u}$表示流体速度，
        $p$表示压力。对流项$\vec{u} \cdot \nabla \vec{u} := (\vec{u} \cdot \nabla) \vec{u}$，是非
        线性项，这也使得Navier-Stokes方程的解不唯一，这也给我们的数值计算带来了一定的挑战。
        系统(\ref{eq::NS_system})的计算区域是$\Omega$，可以是二维的或三维的。在边界$\partial \Omega =
        \partial \Omega_D \cup \Omega_N$上的边界条件如下：
        \begin{equation}
            \begin{array}{ll}
                \vec{u} = \vec{w},& \mbox{ on } \partial \Omega_D,\\
                \nu \displaystyle \frac{\partial \vec{u}}{\partial n} - p =
                \vec{0}, & \mbox{ on } \partial \Omega_N.  \\
             \end{array}
            \label{eq::bc}
        \end{equation}
        其中边界$\partial \Omega = \partial \Omega_D \bigcup \partial \Omega_N$,
        $\vec{n}$表示边界的外法向。
        Dirichlet边界根据边界上的速度和外法向的乘积，可以细分为：
        \begin{equation}
            \begin{aligned}
                \partial \Omega_{+} = {x \mbox{在} \partial \Omega \mbox{上}|\vec{\omega} \cdot \vec{n} > 0}, \quad \mbox{出流边界}，\\
                \partial \Omega_{0} = {x \mbox{在} \partial \Omega \mbox{上}|\vec{\omega} \cdot \vec{n} = 0}, \quad \mbox{特征边界}，\\
                \partial \Omega_{-} = {x \mbox{在} \partial \Omega \mbox{上}|\vec{\omega} \cdot \vec{n} < 0}, \quad \mbox{入流边界}，\\
            \end{aligned}
        \end{equation}
        如果边界全部是Direchlet边界条件，即$\partial \Omega = \partial \Omega_D$，那么Navier-Stokes问题
        (\ref{eq::NS_system})和(\ref{eq::bc})的压力解除去一个常数外是唯一的。
        我们对(\ref{eq::NS_system})中的不可压约束应用散度定理，
        \begin{equation}
            0 = \int_{\Omega} \nabla \cdot \vec{u} = \int_{\partial \Omega} \vec{u} \cdot \vec{n} = \int_{\Omega} \vec{\omega} \cdot \vec{n}.
        \end{equation}
        即边界值要满足相容性条件
        \begin{equation}
            \int_{\partial \Omega_{+}} \vec{\omega} \cdot \vec{n} + \int_{\partial \Omega_{-}} \vec{\omega} \cdot \vec{n} = 0.
        \end{equation}
        简单的讲，就是说流入$\Omega$的流体的体积，要与流出的体积相等。这也是压力不唯一的原因。在处理入流/出流的问题时，要注意保
        证这个相容性条件，否则Navier-Stokes问题的解有可能不存在。一般情况下，我们在出流边界设置自然条件，相容性条件会自然满足，因此
        这时候问题(\ref{eq::NS_system})和(\ref{eq::bc})的压力解是唯一的。

        (\ref{eq::NS_system})是Navier-Stokes的原始变量形式,它的另外一组表现形式是流函数形式。如果区域$\Omega$是单连通的，那么不可压缩条件
        \begin{equation}
            \frac{\partial u_x}{\partial x} = -\frac{\partial u_y}{\partial y}
        \end{equation}
        意味着存在一个流函数$\psi(x, y)$ 使得
        \begin{equation}
            u_x = \frac{\partial \psi}{\partial y}, \quad u_y = -\frac{\partial \psi}{\partial x}.
        \end{equation}
        $\psi(x, y)$除去一个常数外被$\vec{u}$唯一确定。
        对(\ref{eq::NS_system})中的动量方程取curl，我们可以得到流函数形式：
        \begin{equation}
            -\nu \nabla^4 \psi + \frac{\partial \psi}{\partial y}\frac{\partial}{\partial x}(\nabla^2\psi) - \frac{\partial \psi}{\partial x}\frac{\partial }{\partial y}(\nabla^2 \psi) = -curl \vec{f}
            \label{eq::stream_line}
        \end{equation}
        但由于流函数形式(\ref{eq::stream_line})在三维空间没有一般形式，所以流函数形式只在二维。我们还是研究(\ref{eq::NS_system})中的原始
        变量形式。
    \subsection{Weak formulation}
        我们定义解和检验空间如下：
        \begin{eqnarray}
            \mathbf{H}_E^1 & := & \left\{ \vec{u} \in \mathcal{H}^1(\Omega)^d \big|
            \vec{u} = \vec{w} \mbox{ on } \partial \Omega_D \right\},\\
            \mathbf{H}_{E_0}^1 & := & \left\{ \vec{v} \in \mathcal{H}^1(\Omega)^d \big|
            \vec{u} = \vec{0} \mbox{ on } \partial \Omega_D \right\}.
        \end{eqnarray}
        那么变分形式为：寻找$(\vec{u}, p) \in (\mathbf{H}_E^1,
        L_2(\Omega))$ 使得
        \begin{eqnarray}
             \int_{\Omega}\frac{\partial \vec{u}}{\partial t} \cdot \vec{v} +
            \nu \int_\Omega \nabla \vec{u} : \nabla \vec{v} + \int_\Omega \left(
            \vec{u} \cdot \nabla \vec{u} \right) \cdot \vec{v} - \int p
            \left( \nabla \cdot \vec{v} \right) & = & \int_\Omega \vec{f} \cdot
            \vec{v}, \label{eq::generalweak_momentum}\\
             & & \forall \vec{v} \in \mathbf{H}_{E_0}^1,\notag \\
             \int_\Omega q \left( \nabla \cdot \vec{u} \right) & = & 0,
             \label{eq::generalweak_mass} \\
            & & \forall q \in L_2(\Omega). \notag
       \end{eqnarray}

       其中$\nabla \vec{u} : \nabla \vec{v}$ 表示纯量的乘积，在二维中为
       $\nabla u_x \cdot \nabla v_x + \nabla u_y \cdot \nabla v_y$.
       我们在构造混合元近似的时候要确保可解性条件
       \begin{equation}
            \int_{\Omega}p \nabla \cdot \vec{v} = 0 \quad \text{对任意的}\vec{v} \in \mathbf{H}_{E_0}^1 \Rightarrow \left\{ \begin{array}{ll}
                p = \mbox{constant} & \text{如果} \partial \Omega = \partial \Omega_D \\
                p = 0 & \text{其他情况}
            \end{array}
            \right.
            \label{eq::solve_condition}
       \end{equation}
        (\ref{eq::solve_condition})可以由inf-sup 条件推出。
        \begin{equation}
            \inf_{ q \neq \mbox{constant}} \sup_{\vec{v} \neq \vec{0}} \frac{|(q, \nabla \cdot \vec{v})|}{||vec{v}||_{H(\Omega)^1} ||q||_{L(\Omega)^0}} \geq \gamma > 0.
        \end{equation}

       假设$\tau_h$是$\Omega$ 上对压力网格的三角剖分，网格尺度 $h = max_{T
         \in \tau_h} diam(T)$, $T$ 为三角剖分 $\tau_h$ 的单元。对应的，
       $\tau_{\frac{h}{2}}$ 是对速度网格的三角剖分。基于
       $\tau_{\frac{h}{2}}$ 和 $\tau_{h}$ 上的有限元空间 $X_E^h$ 和 $P_h$
       满足
       \begin{equation}
         X_E^h \subset \mathcal{H}_E, \quad P_h \subset L_2(\Omega)
         \notag
       \end{equation}

       那么(\ref{eq::generalweak_momentum}) 和 (\ref{eq::generalweak_mass})
       可以写成如下形式: 寻找 $(\vec{u}_h, p_h) \in X_E^h \times P_h$ 使得
       \begin{equation}
         \begin{aligned}
           &\int_{\Omega}\frac{\partial \vec{u}_h}{\partial t} \cdot \vec{v}_h
           + \nu \int_\Omega \nabla \vec{u}_h : \nabla \vec{v}_h  \\
           + & \int_\Omega \left( \vec{u}_h \cdot \nabla \vec{u}_h \right)
           \cdot \vec{v}_h - \int_\Omega p_h \left( \nabla \cdot \vec{v}_h
           \right) & = &\int_\Omega \vec{f} \cdot \vec{v}_h, &\quad
           \forall \vec{v}_h \in \mathbf{X}_0^h;  \\
           & \int_\Omega q_h \left( \nabla \cdot \vec{u}_h \right) & = &0,&
           \quad \forall q_h \in P^h.
           \label{eq::discreted_weak}
         \end{aligned}
       \end{equation}

   为了简便，我们时间离散上用显示Euler格式：
   $\forall (\vec{v}_h, q_h) \in \mathbf{X}_0^h \times P^h$
   \begin{equation}
     \begin{aligned}
       \int_{\Omega} \frac{\vec{u}_h^{(n + 1)} -
         \vec{u}_h^{(n)}}{\delta t} \cdot \vec{v} + \nu \int_{\Omega} \nabla
       \vec{u}_h^{(n + 1)} : \nabla \vec{v}_h - \int_{\Omega} p_h^{(n
         + 1)} \left( \nabla \cdot \vec{v}_h \right)
       & = \int_{\Omega}\left(\vec{f} + \vec{u}_h^{(n)} \cdot \nabla \vec{u}_h^{(n)}
       \right) \cdot \vec{v} &\\
       \int_{\Omega} q_h \nabla \cdot \vec{u}_h^{(n + 1)} & = 0.&
     \end{aligned}
     \label{eq::time_discreted_weak}
   \end{equation}
   令 $\{\phi_j \}_{j = 1}^n$ 和 $\{\psi_k\}_{k = 1}^m$ 分别为速度和压
   力的线性元基函数。则数值解$\vec{u}_h^{(n + 1)} = (u_{xh}^{(n + 1)},
   u_{yh}^{(n + 1)}), p_h$ 可以写成如下形式：
   \begin{equation}
     u_{xh}^{(n + 1)} = \sum_{j = 1}^n u_j \phi_j, \quad u_{yh}^{(n +
       1)} = \sum_{j = 1}^n v_j \phi_j, \quad p_h^{(n + 1)} = \sum_{k
       = 1}^m p_k \psi_k \notag
   \end{equation}
   将$u_{xh}^{(n + 1)}, u_{yh}^{(n + 1)}, p_h^{(n + 1)}$带入离散弱形式
   (\ref{eq::time_discreted_weak}) 中，可以
   得到线性方程组
   \begin{equation}
     \left[
       \begin{array}{lll}
         \frac{1}{dt} M + \nu A & 0 & B_x^T \\
         0 & \frac{1}{dt} M +\nu A  & B_y^T \\
         B_x & B_y & 0
       \end{array}
     \right]
     \left[
       \begin{array}{c}
         u_x \\
         u_y \\
         p
       \end{array}
     \right] =
     \left[
       \begin{array}{c}
         f_x \\
         f_y \\
         g
       \end{array}
     \right],
     \label{eq::linear_system}
   \end{equation}
   其中M 是 $n \times n$ 的质量矩阵，A是拉普拉斯矩阵，由以下形式：
   \begin{eqnarray}
     A & = & [a_{ij}], \quad a_{ij} = \int_\Omega \nabla \phi_i \cdot \nabla
     \phi_j  \notag \\
     M & = & [m_{ij}], \quad m_{ij} = \int_{\Omega}\phi_i \phi_j \notag \\
     B_x^T & = & [bx^T_{ik}], \quad bx^T_{ik} = \int_{\Omega} \psi_k
     \frac{\partial{\phi_i}}{\partial x}  \notag \\
     B_y^T & = & [by^T_{ik}], \quad by^T_{ik} = \int_{\Omega} \psi_k
     \frac{\partial {\phi_i}}{\partial y}  \notag \\
     f_x & = & [f_i], \quad f_i = \int_\Omega (\frac{u_{xh}^{(n)}}{dt} - (u_{xh}^{(n)}
     \frac{\partial u_{xh}^{(n)}}{\partial x} + u_{yh}^{(n)} \frac{\partial
       u_{xh}^{(n)}}{\partial y}) \phi_i \notag \\
     f_y & = & [f_i], \quad f_i = \int_\Omega (\frac{u_{yh}^{(n)}}{dt} - (u_{xh}^{(n)}
     \frac{\partial u_{yh}^{(n)}}{\partial x} + u_{yh}^{(n)} \frac{\partial
       u_{yh}^{(n)}}{\partial y}) \phi_i \notag \\
     g & = &  [g_k],\quad  g_i = 0. \notag
   \end{eqnarray}

   接下来我们着重解释一下散度块矩阵$B_x B_y, B_x^T, B_y^T$的拼装。 以
   $B_x^T$为例, 令$\triangle_{v_i}$ 为一个速度单元，我们可以通过上一节建立的
   单元索引来找到对应的压力单元 $\triangle_{p_k}$(如 Figure
   (\ref{fig::matrix_assemble})). $\phi_{i_1}, \phi_{i_2}, \phi_{i_3}$
   是定义在速度单元 $\triangle_{v_i}$ 上的线性元基函数，下标 $i_1,
   i_2, i_3$ 表示该顶点上的自由度在速度全部自由度的全局编号。同时
   $\psi_{k_1}, \psi_{k_2}, \psi_{k_3}$ 是定义在压力单元
   $\triangle_{p_k}$ 上的线性元基函数，$k_1, k_2, k_3$ 代表单元
   $\triangle_{p_k}$ 上的自由度在压力全部自由度中的全局编号。
   因此 单元 $\triangle_{p_k}$ 和 $\triangle_{v_i}$ 对 $B_x^T$ 的贡献
   为
   \begin{equation}
     \left[
     \begin{array}{lll}
       \int_{\triangle_{v_i}}\psi_{k_1} \frac{\partial
         \phi_{i_1}}{\partial x} & \int_{\triangle_{v_i}} \psi_{k_2}
       \frac{\partial \phi_{i_1}}{\partial x} & \int_{\triangle_{v_i}}
       \psi_{k_3} \frac{\partial \phi_{i_1}}{\partial x} \\
       \int_{\triangle_{v_i}} \psi_{k_1} \frac{\partial
         \phi_{i_2}}{\partial x} & \int_{\triangle_{v_i}} \psi_{k_2}
       \frac{\partial \phi_{i_2}}{\partial x} & \int_{\triangle_{v_i}}
       \psi_{k_3} \frac{\partial \phi_{i_2}}{\partial x} \\
       \int_{\triangle_{v_i}} \psi_{k_1} \frac{\partial
         \phi_{i_3}}{\partial x} & \int_{\triangle_{v_i}} \psi_{k_2}
       \frac{\partial \phi_{i_3}}{\partial x} & \int_{\triangle_{v_i}}
       \psi_{k_3} \frac{\partial \phi_{i_3}}{\partial x}
     \end{array}
   \right]
   \label{eq::element_matrix}
   \end{equation}

   将贡献矩阵 (\ref{eq::element_matrix}) 加到 $B_x^T$ 中的相应位置
   $(i_1,k_1)$, $(i_1, k_2)$, $(i_1, k_3)$, $(i_2, k_1)$, $(i_2, k_2)$,
   $(i_2,k_3)$, $(i_3, k_1)$, $(i_3, k_2)$, $(i_3, k_3)$ to  $B_x^T$.
   上。通过遍历所有的速度单元，重复上面的操作，$B_x^T$ 拼装完成。
   $B_y^T$ 也可以同样拼装。

   $B_x$ 和 $B_y$ 的拼装是遍历所有的压力单元，通过单元索引，找到对应的
   四个速度单元，然后分别用压力单元和四个速度单元进行拼装。过程相似，
   便不再赘述。
   \begin{figure}
     \centering
     \begin{tikzpicture}
       % 三角形
       \draw (2, 1) -- (4, 1) -- (3, 3) -- cycle;
       % 三个顶点
       \draw (2, 1) circle(0.05cm);
       \draw (4, 1) circle(0.05cm);
       \draw (3, 3) circle(0.05cm);
       \draw (3, 2) node{$p_k$};
       % 三个顶点标号.
       \draw (1.7, 1) node {$k_2$};
       \draw (4.3, 1) node {$k_1$};
       \draw (3, 3.3) node {$k_3$};
       % 四个三角形
       \draw (5, 1) -- (6, 3) -- (7, 1) -- cycle;
       \draw (5.5, 2) -- (6.5, 2) -- (6, 1) -- cycle;
       % 六个顶点
       \draw [fill = black](6.5, 2) circle(0.05cm);
       \draw [fill = black](5.5, 2) circle(0.05cm);
       \draw [fill = black](6, 3) circle(0.05cm);
       % 三个顶点标号.
       \draw (6.8, 2) node {$i_1$};
       \draw (6.3, 3.3) node {$i_2$};
       \draw (5.2, 2) node {$i_3$};
       % 单元标记.
       \draw (6, 2.5) node {$v_i$};
       \draw (5.5, 1.5) node {$v_j$};
       \draw (6.5, 1.5) node {$v_p$};
       \draw (6, 1.5) node {$v_q$};
     \end{tikzpicture}
     \caption{左: $p$ 单元; 右: 与压力单元对应的4个速度$v$单元 }
     \label{fig::matrix_assemble}
   \end{figure}

   我们主要的想法是有两套网格，速度网格和压力网格，速度网格可以由压力
   网格加密一次得到。因此我们对压力网格进行移动，然后对移动完的压力网
   格全局加密一次，即可实现对速度网格的同步移动。
   为了更好的说明我们的数值方法，我们将算法的流程图如Algorithm(\ref{alg::solve})所示.
   \begin{algorithm}
     \caption{移动网格方法来求解Navier Stokes方程}
     \begin{algorithmic}[1]
       \State 在初始速度网格$\triangle_v^{(0)}$和压力网格
       $\triangle_p^{(0)}$上，求解$t = t_n$时刻Stokes方程(\ref{eq::stokes}), 获
       得数值解$\vec{u}_h^{(0)}, p_h^{(0)}$.
       \While {$t_n < T$}
             \State 在压力网格$\triangle_p^{(n)}$上，用$\vec{u}_h^{(n)},
                    p_h^{(n)}$来计算控制函数, 并通过求解
                    (\ref{eq::logical}), 获得$\vec{\xi}^*$. \label{state::monitor}
             \State 判断$\parallel \vec{\xi}^* -
                    \vec{\xi}^{(0)} \parallel_{L^2}$ 是否小于容忍量
                    $\epsilon$, 如果是迭代结束，否则，继续做
                    \ref{state::start} - \ref{state::end}.
             \State 用$\vec{\xi}^* - \vec{\xi}^{(0)}$两者之差来计算
                    网格$\triangle_p^{(n)}$的移动量$\delta \vec{x}$.
                    \label{state::start}
             \State 利用\ref{state::start}中$\delta \vec{x}$, 在速度网格
                    $\triangle_v^{(n)}$上求解更新数值解的方程
                    (\ref{eq::continous_update}), 得到新网格上的中间量
                    $\vec{u}_{h, *}^{(n)}, p_{h, *}^{(n)}$.
             \State 更新$\triangle_p^{(n)}$, 通过几何遗传树结构，来
                    同步$\triangle_v^{(n)}$, 得到新的$\triangle_p^{(n
                    + 1)}$和$\triangle_v^{(n + 1)}$
             \State 回到\ref{state::monitor}. \label{state::end}
             \State 在$\triangle_v^{(n + 1)}$和$\triangle_p^{(n + 1)}$
                    上求解Navier-Stokes方程
                    (\ref{eq::linear_system}).从而真正获得$t = t_{n +
                      1}$时刻的数值解$\vec{u}_h^{(n + 1)}, p_h^{(n +
                      1)}$.
             \State $n = n + 1$
        \EndWhile
     \end{algorithmic}
     \label{alg::solve}
   \end{algorithm}
\section{移动网格策略}

   在 $t = t_{n + 1}$ 时刻， 用上一章的方法可以得到有限元解
   $(\vec{u}_h^{(n + 1)}, p_h^{(n + 1)})$. 下面的问题是如何用新的数值
   解和旧的网格$\mathcal {T}_h^{(n)}$ 来获得新的网格$\mathcal {T}_h^{(n +
   1)}$. 我们采取文献 \cite{di2005moving} 中的方法，注意到我们区域的边
   界均是Dirichlet边界，分为以下四步：
   \subsection{Step 1 获取Monitor}
     选择一个合适的控制函数，对于移动网格的结果是非常重要的。应用在不
     可压Navier-Stokes方程上的控制函数主要有如下几种.
     令 $m = \frac{1}{G}$ 其中$m$是(\ref{eq::logical})中的一个纯量函数。
     关于$G$有集中不同的选择.一种是基于涡量
     \begin{equation}
       G_0 = \sqrt{1 + \alpha |\omega|^\beta}
       \label{eq::monitor_vorticity}
     \end{equation}
     其中$\omega = \nabla \times \vec{u}$, $\alpha$和$\beta$是两个正的
     常数。

      另外一个选择，$G$是基于数值解的梯度
      \begin{equation}
        G_1 = \sqrt{1 + \alpha |\nabla \vec{u}|^\beta}
        \label{eq::monitor_gradient}
      \end{equation}

      对于线性元$v_h$逼近真实解$v$，下面的后验误差估计公式可以用来近似
      计算误差
      \begin{equation}
        |v - v_h|_{1, \Omega} \sim \eta(v_h) := \sqrt{\sum\limits_{l:
            \text{内部边界}} \int_l \left[ \nabla v_h \cdot \vec{n}_l
            \right]^2 dl}
      \end{equation}
      其中$[\cdot]_l$意味着边$l$上的跳跃，即$[v]_l = v|_{l^{+}} -
      v|_{l^{+}}$. 很自然的在每个单元上等分布数值误差$\eta(v_h)$, 控制
      函数为一下形式
      \begin{equation}
        G_2 = \sqrt{1 + \alpha \eta^2(v_h)}
        \label{eq::monitor_posterror}
      \end{equation}
      \cite{di2005moving}中对(\ref{eq::monitor_posterror})进行了改进
      \begin{equation}
        G_3 = \sqrt{1 + \alpha \left[ \eta(v_h) / \text{max}\eta(v_h)
          \right]^\beta}
        \label{eq::monitor_posterror_modified}
      \end{equation}
      其中$\beta > 2$时有更好的效果。

   \subsection{Step 2 获取新的逻辑网格}
     求解椭圆形方程
     \begin{eqnarray}
       \begin{aligned}
         \nabla_{\vec{x}} \left ( m \nabla_{\vec{x}} \vec{\xi} \right)
         = 0 \\
         \vec{\xi} = \vec{\xi}_b
       \end{aligned}
       \label{eq::logical}
     \end{eqnarray}
     其中 $m$ 是上一节中的纯量函数，通常依赖于$(\vec{u}_h^{(n + 1)},
     p_h^{(n + 1)})$。我们定义初始的逻辑网格$\mathcal{T}_c (\mathcal
     {A}^{0} \text{为它的节点})$。一旦初始的逻辑网格给定，在整个的求解
     过程中，将一直保持不变。通过求解(\ref{eq::logical}) 我们可以得到
     新的逻辑网格$\mathcal{T}_c^*$($\mathcal{A}^*$为它的节点)。
  \subsection{Step 3 物理网格的移动方向}

     我们先引入一些定义。$\mathcal{T}_h$ 为物理区域上的三角剖分。第i个
     点定义为$X_i$，以$X_i$ 为顶点的单元的集合称之为$T_i$。相应的计算
     区域上的标记为 $\mathcal{T}_c, \mathcal{A}_i$ 和 $T_{i,c}$。
     $\mathcal{A}_i$ 点在计算区域上的坐标定义为 $(\mathcal{A}_i^1,
     \mathcal{A}_i^2)^T$。在 Step 1 结束后，我们得到了新的逻辑网格
     $\mathcal{T}_c^*$ 和它的顶点 $\mathcal{A}_i^*$. 从而我们得到新旧
     逻辑网格的差:
     \begin{equation}
       \delta \mathcal{A}_i  = \mathcal{A}^{(0)} - \mathcal{A}_i^*
     \end{equation}
     对于一个给定的单元$E \in \mathcal{T}_h$, $X_{E_k}, 0 \leq k \leq
     2 $, 作为它的三个顶点。从$V_{\mathcal{T}_c^*}(\Omega)$ 到
     $V_{\mathcal{T}}(\Omega)$ 的分片线性映射在单元 $E$ 上的梯度是常数，
     并且满足下面的方程组：
     \begin{eqnarray}
       \begin{aligned}
        & \left (
           \begin{array}{cc}
             \mathcal{A}_{E_1}^{*, 1} - \mathcal{A}_{E_0}^{*, 1} &
             \mathcal{A}_{E_2}^{*, 1} - \mathcal{A}_{E_0}^{*, 1} \\
             \mathcal{A}_{E_1}^{*, 2} - \mathcal{A}_{E_0}^{*, 2} &
             \mathcal{A}_{E_2}^{*, 2} - \mathcal{A}_{E_0}^{*, 2}
           \end{array}
         \right )
         \left (
           \begin{array}{cc}
             \frac{\partial x^1}{\partial \xi^1} & \frac{\partial
               x^1}{\partial \xi^2} \\
             \frac{\partial x^2}{\partial
               \xi^1} & \frac{\partial x^2}{\partial \xi^2}
           \end{array}
         \right ) \notag \\ = &
         \left (
           \begin{array}{ll}
             X_{E_1}^1 - X_{E_0}^1 & X_{E_2}^1 - X_{E_0}^1 \\
             X_{E_1}^2 - X_{E_0}^2 & X_{E_2}^2 - X_{E_0}^2
           \end{array}
         \right )
       \end{aligned}
     \end{eqnarray}

  求解上面的方程组，可以获得单元 $E$ 上的$\partial \vec{x} / \partial
  \xi$. 如果以单元的面积作为权重，则第i个点的加权平均的位移定义如下:
  \begin{eqnarray}
    \delta X_i = \frac{\sum\limits_{E \in T_i} |E| \frac{\partial
        \vec{x}}{\partial \xi}|_{\text{in} E} \delta
      \mathcal{A}_i}{\sum\limits_{E \in T_i} |E|}.
  \end{eqnarray}
  其中$|E|$代表单元$E$的面积. 为了避免网格发生缠结，在网格移动向量前乘
  上一个常量$\mu$, 即物理区域上新网格$\mathcal{T}^*$的节点表示为：
  \begin{equation}
    X_i^* = X_i + \mu \delta X_i.
  \end{equation}
  文献\cite{di2005moving}中提出$\mu$按以下方式给出:
  \begin{equation}
    \left |
      \begin{array}{ccc}
        1 & 1 & 1 \\
        x_0^1 + \mu \delta x_0^1 & x_1^1 + \mu \delta x_1^1 & x_2^1 +
        \mu \delta x_2^1 \\
        x_0^2 + \mu \delta x_0^2 & x_1^2 + \mu \delta x_1^2 & x_2^2 +
        \mu \delta x_2 ^2
      \end{array}
    \right | = 0
    \label{eq::muValue}
  \end{equation}
  其中$ \vec{x}_i = (x_i^1, x_i^2), 0 \leq i \leq 2$表示第i个点的坐标。
  令$\mu_i^*$ 为方程(\ref{eq::muValue})的最小正根，则令
  \begin{equation}
    \mu = \text{min} (1, \frac{\mu_i^*}{2}).
  \end{equation}

  \subsection{Step 4 散度为0的插值}
    用移动网格方法求解不可压流体时，要保证插值的过程散度是为0的。通过
    求解一个对流方程，对流的速度是网格的移动速度，从而实现旧的物理网格
    上的数值解到新的物理网格上数值解的插值。令$u_h = \sum u_i \phi_i,
    u_h \in \mathcal{X}_E^h$， $\phi_i$是有限元空间$\mathcal{X}_h$的
    基函数。引入一个虚拟的时间$\tau$, 假设基函数$\phi_i$和$u_i$均是关
    于$\tau$的函数,即$\phi_i = \phi_i(\tau), u_i = u_i(\tau)$.
    我们引入一个从旧网格$x^{\text{旧}}$到新网格$x^{\text{新}}$网格点的
    连续变换：
    \begin{equation}
       x_i(\tau) = X_i + \tau (X_i^* - X_i), \qquad \tau \in [0, 1]
       \label{eq::mesh_old2new}
    \end{equation}
    其中$X_i^* = x_i^{\text{新}}, X_i = x_i^{\text{旧}}$
    基于(\ref{eq::mesh_old2new})的连续形式$x(\tau) = x_{\text{旧}} +
    \tau (x^{\text{新}} - x^{\text{旧}})$，基函数可以定义为
    $\phi_i(\tau) = \phi_i(x(\tau))$ 并且 $u_i = u_i(x(\tau))$.

    在插值的过程中，我们要保持解曲线$u_h = \sum u_i \phi_i$关于$\tau$
    在弱形式下是不变的.即对$\forall \psi \in \mathcal{X}_h$,
    $(\partial_{\tau} u_h, \psi) = 0$。通过直接计算可得
    \begin{equation}
      \frac{\partial \phi}{\partial \tau} = -\nabla_{\vec{x}} \phi_i
      \cdot \delta \vec{x}
    \end{equation}
    其中$\delta \vec{x} = x^{\text{旧}} - x^{\text{新}}$。紧接着
    \begin{eqnarray}
      \begin{aligned}
        0 & = (\partial_{\tau} u_h , \psi) \\
        & = (\partial_{\tau} \sum u_i(x(\tau)) \phi_i, \psi) \\
        & = (\sum \phi_i \partial u_i(x(\tau)) + \sum u_i \partial_{\tau}
        \phi_i) \\
        & =  (\sum \phi_i \partial_{\tau} u_i(x(\tau)) -\sum u_i
        \nabla_{\vec{x}}\phi_i \cdot \delta \vec{x}, \psi) \\
        & =  (\sum \phi_i \partial_{\tau} u_i(x(\tau)) - \nabla_{\vec{x}}u_h
        \cdot \delta \vec{x}, \psi)
     \end{aligned}
     \label{eq::divergence_free}
    \end{eqnarray}
    我们将(\ref{eq::divergence_free})应用到不可压流上，即速度场要满足
    散度为0的条件。令$\mathcal{X}_h$ 为散度为0的空间：
    \begin{equation}
      \mathcal{X}_E^h = X_E^h \cap \{\vec{u}_h|\nabla \cdot \vec{u}_h
      = 0 \}
    \end{equation}
    那么(\ref{eq::divergence_free})将变成：寻找$w_h \in
    \mathcal{X}_h$ 使得
    \begin{equation}
      \left( \sum \phi_i \partial_{\tau} u_i - \sum u_i \nabla_{\vec{x}}
      \phi_i \cdot \delta \vec{x}, z_h \right) = 0 \quad \forall z_h
      \in \mathcal{X}_h.
      \label{eq::div_free_space}
    \end{equation}
    上面的结果意味着
    \begin{equation}
      \sum \phi_i \partial_{\tau} u_i - \sum u_i \nabla_{\vec{x}}
      \phi_i \cdot \delta \vec{x} \in \mathcal{X}_h^{\perp}
    \end{equation}
    其中$\mathcal{X}_h^{\perp} + \mathcal{X}_h = L^2$. 根据文献
    \cite{gunzburger2012finite}中的定理2.7, 如果区域$\Omega$ 是单连通
    的,那么
    \begin{equation}
      \mathcal{X}_h^{\perp} = \{ \nabla q | q \in H^1(\Omega) \}
      \label{eq::orthogonal_space}
    \end{equation}
    则存在$\nabla p \in \mathcal{X}_h^{\perp}$使得
    \begin{eqnarray}
      \begin{aligned}
        \sum \phi_i \partial_{\tau} u_i - \sum u_i \nabla_{\vec{x}}
        \phi_i \cdot \delta \vec{x}  &=  -\nabla p &\\
        \nabla_{\vec{x}} \cdot u_h   &=  0.&
      \end{aligned}
      \label{eq::continous_update}
    \end{eqnarray}

    \begin{remark}
       这里的$p$跟外部Navier-Stokes方程的解p不一致，只是一个辅助量。
    \end{remark}

    (\ref{eq::continous_update})的弱形式：寻找$\left(\vec{u}_h, p_h
    \right) \in X_E^h \times P_h$ 使得
    \begin{eqnarray}
      \begin{aligned}
        \left( \sum \phi_i \partial_{\tau}u_i - \sum u_i
          \nabla_{\vec{x}} \phi_i \cdot \delta \vec{x}, v_h \right)& =
        \left( p_h, \nabla v_h \right),& \quad \forall v_h \in X_E^h. \\
        \left( \nabla_{\vec{x}} \cdot u_h, q_h \right)& = 0, &\quad
        \forall q_h \in P_h
      \end{aligned}
      \label{eq::discreted_update}
    \end{eqnarray}
    (\ref{eq::continous_update})和(\ref{eq::discreted_update})的初值为
    在$t = t_n$时刻的网格上，$t = t_{n + 1}$时刻外部Navier-Stokes方
    程的解。

    时间方向的离散我们暂时先用线性Euler方法:
    \begin{eqnarray}
      \begin{aligned}
        \left ( \frac{\sum \phi_i u_{i, *}^{(n)} - \sum \phi_i
            u_i^{(n)}}{\Delta \tau}, v_h \right) - \left ( \sum
          u_i^{(n)} \nabla \phi_i \cdot \delta \vec{x}, v_h \right) &
        = \left( p_h^{(n)},  \nabla v_h \right).& \\
        \left( \nabla \cdot u_{h, *}^{(n)}, q_h \right) & = 0 &
      \end{aligned}
    \end{eqnarray}
    注意到在做数值解插值的过程中，物理网格还没有发生移动，这时候基函
    数$\phi$仍然是$t = t_n$时刻网格上的基函数。所以$u_{h,*}^{(n)} = \sum
    u_{i, *}^{(n)} \phi_i^{(n)}, \quad u_h^{(n)} = \sum u_i^{(n)}
      \phi_i^{(n)}$简单整理一下：
    \begin{eqnarray}
      \begin{aligned}
        \left ( \frac{u_{h, *}^{(n)} - u_h^{(n)}}{\Delta \tau}, v_h
        \right) - \left (\nabla u_h^{(n)} \cdot \delta \vec{x}, v_h
        \right) & = \left( p_h^{(n)}, \nabla v_h \right).& \\
        \left( \nabla \cdot u_{h, *}^{(n)}, q_h \right) & = 0 &
      \end{aligned}
    \end{eqnarray}
    其中$u_h^{(n)}$ 和$p_h$ 是在$t = t_n$时刻的网格上，$t = t_{n + 1}$
    时刻，Navier-Stokes 方程的数值解。而$u_{h, *}^{(n)}$ 和$p_{h,
      *}^{(n)}$是在新的网格上$t = t_{n + 1}$时刻更新的解。但这组解不能
    当作外部Navier-Stokes方程的解。需要在新网格上重新求解Navier-Stokes
    方程，得到的解才是我们想要的解。
    \section{数值算例}
       \subsection{Colliding Flow}
          这个例子为稳态Stokes方程的精确解，粘性系数$\nu = 1.0$:
          \begin{equation}
            u_x = 20 x y^3; \quad u_y = 5 x^4 - 5 y^4; \quad p = 60 x^2 y - 20
            y^3 + \mbox{constant}.
            \label{eq::colliding}
          \end{equation}
          其中计算区域$\Omega = [-1, -1] \times [1, 1]$, 边界条件全部
          是Dirichlet 条件。这个例子是用来检验移动网格方法的收敛阶，此时
          解比较光滑。从文献\cite{bercovier1979error} 可知，我们期望移
          动网格的收敛阶： 速度有二阶收敛，压力一阶收敛。我们先给出均
          匀网格时，误差的收敛阶，如
          Table(\ref{tab::colliding_uniform_error}) 和
          Table(\ref{tab::colliding_uniform_div_error})所示。

%          \begin{table}[!htbp]
%            \centering
%            \begin{tabular}{ccccccc} \hline
%              网格   & $||\vec{u} - \vec{u}_h ||_{L^2}$ & 误差阶 &$||\vec{u} -
%              \vec{u}_h ||_{H^1}$ & $||p - p_h||_{L^0}$ & 误差阶 &$||p -
%              p_h||_{H^1}$  \\ \hline
%              $10 \times 10$   &   $1.42 \times 10^{-1}$   &  &  $3.65 \times
%              10^0$     &   $1.26 \times 10^0$ & & $2.06 \times 10^1$    \\
%              $20 \times 20 $   &   $3.54 \times 10^{-2}$   & 2.01  &  $1.81 \times
%              10^0$     &   $3.90 \times 10^{-1}$ & 1.62 & $1.22 \times 10^1$   \\
%              $40 \times 40 $   &   $8.82 \times 10^{-3}$   & 2.01  & $9.03 \times
%              10^{-1}$  &   $1.14 \times 10^{-1}$ & 1.71 & $6.72 \times 10^0$   \\
%              $80 \times 80 $   &   $2.20 \times 10^{-3}$   & 2.00  & $4.51 \times
%              10^{-1}$  &   $3.39 \times 10^{-2}$ &  1.68 & $3.90 \times 10^0$  \\ \hline
%            \end{tabular}
%            \caption{\small 用均匀网格计算碰撞流的误差, $\nu = 1.0$.}
%            \label{tab::colliding_uniform_error}
%          \end{table}
%
%          \begin{table}[!htbp]
%            \centering
%            \begin{tabular}{ccccc} \hline
%              网格   & 涡量$L^2$误差 & 误差阶 & 散度$L^2$误差 & 误差阶\\ \hline
%              $10 \times 10$    &   $2.62 \times 10^{0}$   &  &   $2.54 \times
%              10^0$ &  \\
%              $20 \times 20 $   &   $1.31 \times 10^{0}$  & 1.00  &   $1.25 \times
%              10^0$ & 1.02 \\
%              $40 \times 40 $   &   $6.54 \times 10^{-1}$ & 1.00  &   $6.22 \times
%              10^{-1}$ &  1.00 \\
%              $80 \times 80 $   &   $3.27 \times 10^{-1}$ & 1.00  &   $3.10 \times
%              10^{-1}$ & 1.00 \\ \hline
%            \end{tabular}
%            \caption{\small 用均匀网格计算碰撞流的散度和涡量误差, $\nu = 1.0$.}
%            \label{tab::colliding_uniform_div_error}
%          \end{table}

          \begin{table}[!htbp]
            \centering
            \begin{tabular}{ccccccc} \hline
              网格   & $||\vec{u} - \vec{u}_h ||_{L^2}$ & 误差阶 &$||\vec{u} -
              \vec{u}_h ||_{H^1}$ & $||p - p_h||_{L^0}$ & 误差阶 &$||p -
              p_h||_{H^1}$  \\ \hline
              $10 \times 10$   &   $1.18 \times 10^{-1}$   &  &  $3.37 \times
              10^0$     &   $8.49 \times 10^{-1}$ & & $1.84 \times 10^1$    \\
              $20 \times 20 $   &   $2.94 \times 10^{-2}$   & 2.01  &  $1.67 \times
              10^0$     &   $2.330 \times 10^{-1}$ & 1.82 & $1.06 \times 10^1$   \\
              $40 \times 40 $   &   $7.37 \times 10^{-3}$   & 1.99  & $8.35 \times
              10^{-1}$  &   $6.54 \times 10^{-2}$ & 1.78 & $5.89 \times 10^0$   \\
              $80 \times 80 $   &   &  &  & &  &  \\ \hline
            \end{tabular}
            \caption{\small 用移动网格计算碰撞流的误差, $\nu = 1.0$.}
            \label{tab::colliding_moving_error}
          \end{table}

          \begin{table}[!htbp]
            \centering
            \begin{tabular}{ccccc} \hline
              网格   & 涡量$L^2$误差 & 误差阶 & 散度$L^2$误差 & 误差阶\\ \hline
              $10 \times 10$    &   $2.35 \times 10^{0}$   &  &   $2.41 \times
              10^0$ &  \\
              $20 \times 20 $   &   $1.16 \times 10^{0}$  & 1.01  &   $1.20 \times
              10^0$ & 1.00 \\
              $40 \times 40 $   &   $5.79 \times 10^{-1}$ & 1.00  &   $6.01 \times
              10^{-1}$ &  1.00 \\
              $80 \times 80 $   &  &   & &  \\ \hline
            \end{tabular}
            \caption{\small 用移动网格计算碰撞流的散度和涡量误差, $\nu = 1.0$.}
            \label{tab::colliding_moving_div_error}
          \end{table}

          我们采用上面一章中的移动网格方法来求解这个算例。选取
          (\ref{eq::monitor_gradient})为控制函数，其中$\vec{u} = (u_x,
          u_y)^T$. $\alpha$和$\beta$分别取为$0.002$和$2$. 从
          Table(\ref{tab::colliding_moving_error})中可以看出速度$L^2$
          误差有二阶，压力收敛阶是一阶。从
          Table(\ref{tab::colliding_moving_div_error})可以看出速度散度
          和涡量均有一阶收敛。

          首先我们判断网格是不是往正确的方向移动。从
          Figure(\ref{fig::mesh_move_direction})中看出控制函数$G_1$最大
          的地方分布在区域的四个顶角上，中间区域控制函数的值是比较小的。
          网格应该从控制函数小的地方移动到控制函数值大的地方。而网格的
          移动方向也确实是往四个顶角上移动。因此我们可以确定网格移动方
          向是对的。再者，判断选取$G_1$为控制函数是否合适。从Figure
          (\ref{fig::uniform_vs_moving_err})中看到：在均匀网格下，速度
          $L^2$误差最大的地方分布在四个顶角上，这与控制函数的分布是一
          致的。所以选取$G_1$为控制函数是合理的。
          \begin{figure}[ht]
            \begin{center}
              \includegraphics[width = 0.43\textwidth, angle = -90]{../picture_collidingFlow/mesh_move_direction.eps}
              \includegraphics[width = 0.43\textwidth, angle = -90]{../picture_collidingFlow/pressure_contour.eps}
              \caption{\small 左：$m = \frac{1}{G_1}$的等高线和网格移
                动方向；右：压力等高线和速度的流速线 $\alpha = 0.002, \beta = 2.0$.}
              \label{fig::mesh_move_direction}
            \end{center}
          \end{figure}

          \begin{figure}[ht]
            \begin{center}
              \includegraphics[width = 0.43\textwidth, angle = -90]{../picture_collidingFlow/uniform20_error.eps}
              \includegraphics[width = 0.43\textwidth, angle = -90]{../picture_collidingFlow/moving20_error.eps}
              \caption{\small 速度$L^2$误差分布，左：均匀网格；右：移动网格 $\alpha = 0.002, \beta = 2.0$.}
              \label{fig::uniform_vs_moving_err}
            \end{center}
          \end{figure}



          \begin{figure}[ht]
            \begin{center}
              \includegraphics[width = 0.43\textwidth, angle = -90]{../picture_collidingFlow/uniform_mesh20.eps}
              \includegraphics[width = 0.43\textwidth, angle = -90]{../picture_collidingFlow/moving_mesh20.eps}
              \caption{\small 网格对比，左：均匀网格$20 \times 20$；右：移动网格 $\alpha = 0.002, \beta = 2.0$.}
              \label{fig::uniform_vs_moving_mesh}
            \end{center}
          \end{figure}

          
\section{Introduction}

Let integer $p \geq 2$. A matrix $X \in \CS^{n \times n}$ is called
a $p$th root of a matrix $A \in \CS^{n \times n}$ if $X^p = A$. If
$A$ has no eigenvalues on $\RS^-$, the closed negative real axis,
and all zero eigenvalues of $A$ are semisimple, there exists a
unique principal $p$th root of $A$, denoted by $A^{1/p}$
\cite{Lin2010}. Eigenvalues of $A^{1/p}$ have argument less in
modulus than $\pi/p$. An application of $p$th root is in the
computation of the matrix logarithm through the relation $\log A = p
\log A^{1/p}$, where $p$ is chosen so that $A^{1/p}$ can be well
approximated by a polynomial or rational function. One can see
\cite{Higham2008} or the recent survey paper \cite{HighamAlMohy2010}
for more applications.

There are two various ways to deal with the matrix $p$th root. The
first way is to use the direct Schur decomposition of $A$, say
$Q^*AQ = R$, to solve the equation $Y^p = R$, instead of the
equation $X^p = A$, by appropriate recurrences on the elements of
$Y$, see for example
\cite{Smith2003,GrecoIannazzo2010,Iannazzo2013}. The second way is
to use the matrix iterations which converge to the principal $p$th
root of $A$, see for example
\cite{BiniHighamMeini2005,GuoHigham2006,Guo2010,Iannazzo2006,Iannazzo2008,
Laszkiewicz2009,Lin2010,Gomilko2012,Zietak2013}.

In this paper, we are interested in the method of matrix iterations.
They are mainly deduced from iterative method for solving nonlinear
equation. When Newton's method is applied on the matrix equation
\begin{equation}
\label{eq:f(X)=0} f(X) = X^p - A = 0,
\end{equation}
Iannazzo shown that the matrix sequence $\{X_k\}$ generated by
Newton's method starting from the identity matrix converges to the
principal $p$th root $A^{1/p}$ if all the eigenvalues of $A$ are in
the set $\MCD = \{z \in \CS: |z| \leq 1, \Real z > 0\}$ or in the
set $\mathcal {E} = \{z \in \CS: 0 < |z| \leq 2, |\arg(z)| <
\pi/4\}$ in \cite{Iannazzo2006} and \cite{Iannazzo2008},
respectively. Subsequently, Guo obtained in \cite{Guo2010} that the
matrix sequence converges to the principal $p$th root of $A$ also
when the eigenvalues of $A$ lie in the set $\mathcal {F} = \{z \in
\CS: |z - 1| \leq 1\}$.


The convergence study for Halley's method applied to
(\ref{eq:f(X)=0}) is studied in \cite{Iannazzo2008} where Iannazzo
proved that the matrix sequence $\{X_k\}$ generated by Halley's
method starting from the identity matrix converges to the principal
$p$th root $A^{1/p}$ for each $A$ having eigenvalues in the set
$\mathcal {G} = \{z \in \CS: \Real z > 0\}$. Guo further proved the
order of convergence is quadratic and cubic for Newton iteration and
Halley iteration in \cite{Guo2010}, respectively.


Methods based on Pad\'{e} approximation (Pad\'{e} family of
iterations and dual Pad\'{e} family of iterations) for computing the
principal $p$th root are also investigated by several authors, see
for example \cite{Laszkiewicz2009, Gomilko2012,Zietak2013}. Note
that Newton's method and Halley's method are special cases as dual
Pad\'{e} family of iterations. Meanwhile, Halley's method is also a
special case of Pad\'{e} family of iterations, see \cite{Zietak2013}
for more details.


As you known, Euler's method is also a famous iteration for solving
nonlinear operator equation.  Euler's method for (\ref{eq:f(X)=0})
in classical form
\begin{equation*}
X_{k + 1} = X_k - [\IO + L_f(X_k)] \inv{f'(X_k)} f(X_k), \ \ k = 0,
1, 2, \ldots,
\end{equation*}
where $\IO$ denotes the identity operator and operator $L_f(X) =
\frac{1}{2} \inv{f'(X)} f''(X) \inv{f'(X)} f(X)$, can be rewritten
as follows:
\begin{equation}
\label{it:EM} X_{k + 1} = E(X_k), \ \ k =0, 1, 2,  \ldots,
\end{equation}
provided $X_0$ commutes with $A$ and $X_k$ is nonsingular for any $k
\geq 0$, where
\begin{equation}
\label{it:EM_fun} E(X) = \displaystyle\frac{1}{2p^2} X \left[(2 p^2
- 3 p + 1)\I + 2(2 p - 1)AX^{-p} - (p - 1)\left(
AX^{-p}\right)^2\right]
\end{equation}
for nonsingular matrix $X \in \CS^{n \times n}$.


A natural question is, how it should be when we apply this method to
solve the principal $p$th root of $A$? It becomes the main goal of
this paper. It is worth noting that, Newton's method and Euler's
method (\ref{it:EM}) are  special cases as the family of
Schr\"{o}der iteration which has recently been studied for the $p$th
root of complex numbers, see for example
\cite{Cardoso2011,Cardoso2011a}.


In the case of scalar Newton iteration  the only existing fixed
points are the $p$th roots of $\lambda$. For scalar Halley
iteration, it has not only the $p$th roots of $\lambda$ as the fixed
points, but has the extra fixed point $z = 0$. While for Euler's
method (\ref{it:EM}), the extra fixed points are $z = 0$ as well as
the $p$th roots of $(p-1)\lambda/(3p-1)$. Based on the above
intrinsic properties, we cannot get the convergence region of
Halley's method obtained in \cite{Iannazzo2008} for Euler's method
(\ref{it:EM}). However, we determine a certain convergence region
for Euler's method (\ref{it:EM}) including that one for Newton's
method given in \cite{Guo2010}. Furthermore, we also analyze the
robust of Euler's method (\ref{it:EM}) and give a modification based
on Schur decomposition which has been applied to Newton and Halley's
methods in \cite{GuoHigham2006} and \cite{Iannazzo2008},
respectively. Numerical experiments illustrate that the modified
algorithm has good numerical properties  as the existing algorithms
and confirm that the Euler method is a good choice for solving the
matrix $p$th root.





This paper is organized as follows. In Section 2, we state our main
result on the convergence of Euler's method (\ref{it:EM}) for
computing the matrix $p$th root. And then we prove this result in
Section 3. In Section 4, we describe our general algorithm and
discuss some related computational issues. Finally in Section 5 we
present some numerical experiments and compare our method with
others existing methods. These results confirm the numerical
stability the overall good performance of the new algorithm.




\section{Convergence results for Euler's method}


Throughout the whole paper, we always suppose that the integer $p
\geq 2$. To state the convergence results for Euler's method
(\ref{it:EM}), we introduce some notations. Let
\begin{equation*}
\label{fun:u(z)} u(z) := \frac{1}{1 - \frac{1}{p}z -
\frac{1}{2}\left(\frac{1}{p} - \frac{1}{p^2}\right)z^2}, \ \ \ z \in
\MCD_0
\end{equation*}
and
\begin{equation}
\label{fun:phi(z)} \phi(z) := 1 - u(z)^p(1 - z), \ \ \  z \in
\MCD_0,
\end{equation}
where
\begin{equation}
\label{set:D0} \MCD_0 := \left\{z \in \CS: |z| < \frac{p}{p -
1}(\sqrt{2p - 1} - 1)\right\}.
\end{equation}


Define
\begin{equation}
\label{set:R} \MCR := 1 - \overline{\MCD}_1 \bigcup \left(\MCD_0
\bigcap \MCD_2\right) := \left\{1-z: z \in \overline{\MCD}_1 \bigcup
\left(\MCD_0 \bigcap \MCD_2\right)\right\}
\end{equation}
and
\begin{equation}
\label{set:R_hat} \widehat{\MCR} := 1 - \mathcal {D}_1 \bigcup
\left(\mathcal {D}_0 \bigcap \mathcal {D}_2\right) := \left\{1-z: z
\in \mathcal {D}_1 \bigcup \left(\mathcal {D}_0 \bigcap \mathcal
{D}_2\right)\right\},
\end{equation}
where $\MCD_0$ is defined in (\ref{set:D0}), $\overline{\MCD}_1$ is
the closure of $\MCD_1$ defined by
\begin{equation}
\label{set:D1} \MCD_1  := \{z \in \CS: |z| < 1\},
\end{equation}
and
\begin{equation}
\label{set:D2} \MCD_2  := \left\{z \in \CS: \sup_{m \geq 3}
\left\{\frac{|S_m(z)|}{|z|}\right\} \cdot \frac{|z| +|\phi(z)|}{|z|
- |\phi(z)|} < 1 \right\},
\end{equation}
where $S_m(z) = \sum\limits_{j = 3}^{m} c_j z^{j}$, $c_j =
\phi^{(j)}(0)/j!, j = 3, 4, \ldots$, and $\phi$ is defined in
(\ref{fun:phi(z)}). That is,
$$
\MCR = \{z\in\CS: 1-z \in \overline{\MCD}_1\} \bigcup \{z\in\CS: 1-z
\in \MCD_0 \bigcap \MCD_2\}.
$$


We have


\begin{theorem}
\label{th:MatrixEMCon} If all eigenvalues of $A \in \CS^{n \times
n}$ are in $\MCR$ defined in $(\ref{set:R})$ and all zero
eigenvalues of $A$ are semisimple, then the matrix sequence
$\{X_k\}$ generated by Euler's method $(\ref{it:EM})$ starting from
$X_0 = \I$ converges to the principal $p$th root $A^{1/p}$.
Moreover, if all eigenvalues are in $\widehat{\MCR}\backslash\{0\}$
defined in $(\ref{set:R_hat})$, then the convergence is cubic.
\end{theorem}



\begin{figure}[p!]
\centering
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p25m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p25m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p25m500.eps}}\\
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p100m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p100m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p100m500.eps}}\\
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p400m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p400m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p400m500.eps}}\\
\caption{The approximating regions of $\mathcal {R}$ defined in
(\ref{set:R}) for $p = 25, 100, 400$ and $m = 20, 100, 500$, where
the red and blue regions denote the set $\{z\in\CS: 1-z \in
\overline{\MCD}_1\}$ and  $\{z\in\CS: 1-z \in \MCD_0 \bigcap
\MCD_2\}$, respectively. }\label{fig:EM_ConvReg}
\end{figure}


\begin{remark}
Figure \ref{fig:EM_ConvReg} plots the approximating region of $\MCR$
with nine cases in the complex plane, where the red and blue regions
denote the sets $\{z\in\CS: 1-z \in \overline{\MCD}_1\}$ and
$\{z\in\CS: 1 - z \in \MCD_0 \bigcap \MCD_2\}$, respectively. Since
$$
\sum_{j = 1}^m c_j^1(1-z)^{j-1} \to \frac{\phi_1(1-z)}{1-z}, \ \ \ n
\to \infty,
$$
for each $z \in \{z\in\CS: 1-z \in \MCD_0\}$ by Lemma
\ref{lem:RatFunTayExp} in Section 3.1, we can see from Figure
\ref{fig:EM_ConvReg} that approximating regions ((a)-(c), (d)-(f),
(g)-(i)) are almost the same for a fixed $p$ when $m=20, 100, 500$,
respectively. To this end, it suffices to choose $m=20$ in practical
numerical computation.
\end{remark}

\begin{remark}
The set $\MCD_0$ defined in (\ref{set:D0}) can be enlarged as
$$
\left\{z \in \CS: \frac{1}{p}|z|\left|1 + \frac{1}{2}(1 -
\frac{1}{p})z\right|<1 \right\}.
$$
However, this improvement does not affect the convergence region
$\MCR$ defined in (\ref{set:R}). So, for convenience, we still use
$\MCD_0$ in our convergence analysis in Section 3.
\end{remark}


\begin{remark}
In practice,  it is not feasible to check whether a eigenvalue
$\lambda$ belongs to $\MCR$. This is due to the computational cost
of (\ref{set:D2}) may be large even if we only choose $m=20$. Thus,
based on the observation from Figure \ref{fig:EM_ConvReg}, we now
give a new convergence region which allows us to determine easily
whether a eigenvalue belongs to it. Define
\begin{equation}
\label{set:R_pra} \MCR_{\text{E}} := \MCD_3 \bigcup \MCD_4,
\end{equation}
where
\begin{align*}
\MCD_3 & := \{z \in \CS: 1-z \in \overline{\MCD}_1 \},\\
\MCD_4 & := \left\{z \in \CS: |\arg(z)| < \frac{\pi}{4}, |1-z| <
\frac{31}{24}\right\}.
\end{align*}
In view of $p(\sqrt{2p - 1} - 1)/(p-1) > 31/24$ for any $p \geq 2$,
one has that $\MCD_4 \subset \MCD_0$. In Figure
\ref{fig:EM_PraConvReg_p100n20}, for $p=100, m=20$, we present the
the regions $\MCR$ and $\MCR_{\text{E}}$ defined in (\ref{set:R})
and (\ref{set:R_pra}), respectively, of Euler's method (\ref{it:EM})
for computing the $p$th root of a matrix. We can observe that the
new region $\MCR_{\text{E}}$ is acceptable approximation to $\MCR$.
So instead of using $\MCR$, in Sections 4 and 5, we will use
$\MCR_{\text{E}}$ in our algorithm and numerical experiments.
\end{remark}


\begin{figure}[t!]
\begin{center}
\scalebox{0.75}[0.75]{\includegraphics{fig_EM_PraConvReg_p100m20.eps}}\\
\end{center}
\caption{For $p=100, m=20$, the actual convergence regions $\MCR$
defined in (\ref{set:R}) (the union of the red and blue parts) and
the approximate convergence regions $\MCR_{\text{E}}$ defined in
(\ref{set:R_pra}) of Euler's method (the yellow parts).}
\label{fig:EM_PraConvReg_p100n20}
\end{figure}



\section{Proof of Theorem \ref{th:MatrixEMCon}}


In this section, we will prove Theorem \ref{th:MatrixEMCon}, the
results on convergence and convergence order for Euler's method
(\ref{it:EM}).


\subsection{Technical lemmas}


The following lemma is taken from \cite[Theorem 3.2]{Cardoso2011}.


\begin{lemma}[\cite{Cardoso2011}]
\label{lem:RatFunTayExp} The Maclaurin series of the function
$\phi(z)$ defined by $(\ref{fun:phi(z)})$ has the form
\begin{equation}
\label{eq:phi(t)_series_form} \phi(z) = \sum_{j = 0}^\infty c_j z^j,
\ \ \ z \in \MCD_0,
\end{equation}
where $\MCD_0$ is defined in $(\ref{set:D0})$, and the coefficients
$c_j = \phi^{(j)}(0)/j!$ satisfying $c_j
> 0$ for all $j \geq 3$ and $\sum\limits_{j = 3}^\infty c_j = 1$.
\end{lemma}



For a complex number $\lambda \in \CS$, let
\begin{equation}
\label{fun:f(z)} f(z) := z^p - \lambda, \ \ \ z \in \CS
\end{equation}
and
\begin{equation}
\label{fun:r(z)} r(z,\lambda) := 1 - \lambda z^{-p}, \ \ \ z \in
\CS\backslash\{0\}.
\end{equation}


\begin{lemma}
\label{lem:r(E(z))_r(z)} Let $r(z,\lambda)$ be defined in
$(\ref{fun:r(z)})$. If $r(z,\lambda) \in \mathcal
{D}_0\backslash\{1\}$ for some $z \in \CS\backslash\{0\}$, where
$\MCD_0$ is defined in $(\ref{set:D0})$, then $E(z)$ generated by
the scalar case of $(\ref{it:EM_fun})$ for $f(z)$ defined in
$(\ref{fun:f(z)})$ exists and
\begin{equation}
\label{eq:r(E(z))_1} r(E(z),\lambda) = \phi(r(z,\lambda)),
\end{equation}
where $\phi$ is defined by $(\ref{fun:phi(z)})$. Moreover, we have
\begin{numcases}{}
|r(E(z),\lambda)| < |r(z,\lambda)|^3, &
if \ $r(z,\lambda) \in \overline{\MCD}_1\backslash\{0,1\}$,\ \ \  \label{ineq:abs_r(E(z))_1}\\
|r(E(z),\lambda)| \leq \sup_{m \geq
3}\left\{\left|\frac{S_m(u)}{u^3}\right|\right\}\cdot \frac{|u| +
|r(z,\lambda)|}{|u| - |r(z,\lambda)|} \cdot |r(z,\lambda)|^3,  & if
\  $r(z,\lambda) \in \mathcal {D}_0\backslash\{1\}$,
\label{ineq:abs_r(E(z))_2}
\end{numcases}
where $\overline{\MCD}_1$ is the closure of $\MCD_1$ defined in
$(\ref{set:D1})$, and $S_m(u)$ is the $m$th partial sum of the
series $(\ref{eq:phi(t)_series_form})$ for each $u \in \mathcal
{D}_0$ satisfying $|u| > |r(z,\lambda)|$, $m \geq 3$.
\end{lemma}




\begin{proof}
For any $z \in \CS\backslash\{0\}$, $E(z)$ exists by
(\ref{it:EM_fun}). Furthermore, when
 $r(z,\lambda) \in \MCD_0\backslash\{0,1\}$, since
\begin{eqnarray*}
\lefteqn{\frac{1}{2p^2}  \left|(2 p^2 - 3 p + 1) + 2(2 p - 1)
\lambda z^{-p} - (p - 1)(\lambda z^{-p})^2\right|} \nonumber\\
& = &  \left|\frac{2p^2 - 3p + 1}{2p^2} + \frac{2p -
1}{p^2}(1 - r(z,\lambda)) - \frac{p - 1}{2p^2}(1 - r(z,\lambda))^2\right| \nonumber \\
& = &  \left|1 - \frac{1}{p} r(z,\lambda) -
\frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)r^2(z,\lambda)\right| \nonumber \\
& > & 1 - \left[\frac{1}{p} |r(z,\lambda)| +
\frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)|r(z,\lambda)|^2\right]\\
&>& 0,\label{eq:zk+1}
\end{eqnarray*}
we know
\begin{equation*}
\label{eq:E(z)} E(z) = \frac{1}{2p^2} z \left[(2 p^2 - 3 p + 1) +
2(2 p - 1) \lambda z^{-p} - (p - 1)(\lambda z^{-p})^2\right] \neq 0
\end{equation*}
and
\begin{equation}
\label{eq:r(E(z))} r(E(z),\lambda) = 1 - \left[1 - \frac{1}{p}
r(z,\lambda) - \frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)r^2(z,\lambda)\right]^{-p}\cdot(1 -
r(z,\lambda)) = \phi(r(z,\lambda)),
\end{equation}
i.e., (\ref{eq:r(E(z))_1}) holds. (\ref{eq:r(E(z))}) shows that
$r(E(z),\lambda) \neq 1$ when $r(z,\lambda) \neq 1$.

If $r(z,\lambda) \in \overline{\MCD}_1\backslash\{0,1\} \subset
\MCD_0$, then, by (\ref{eq:phi(t)_series_form}) in Lemma
\ref{lem:RatFunTayExp} and (\ref{eq:r(E(z))}), one has that
\begin{align}
|r(E(z),\lambda)|  = |\phi(r(z,\lambda))| & = |r(z,\lambda)|^3 \cdot
\left| \sum_{j =
3}^\infty c_j r^{j-3}(z,\lambda) \right| \nonumber\\
& \leq |r(z,\lambda)|^3 \left[|c_3 + c_4 r(z,\lambda)| + \sum_{j =
5}^\infty c_j |r(z,\lambda)|^{j-3}
\right] \nonumber\\
& < |r(z,\lambda)|^3 \sum_{j = 3}^\infty c_j = |r(z,\lambda)|^3 \nonumber\\
& \leq |r(z,\lambda)| \label{ineq:abs_r(E(z))}
\end{align}
from Lemma \ref{lem:RatFunTayExp} and the fact that $|c_3 + c_4 w| <
c_3 + c_4$ for all $w \in \overline{\MCD}_1\backslash\{1\}$. Thus,
(\ref{ineq:abs_r(E(z))_1}) is proved.

If $r(z,\lambda) \in \mathcal {D}_0\backslash\{1\}$, based on
(\ref{eq:phi(t)_series_form}) in Lemma \ref{lem:RatFunTayExp} and
(\ref{eq:r(E(z))}) again, we have
\begin{align*}
r(E(z),\lambda)  = \phi(r(z,\lambda)) = \sum_{j=3}^\infty c_j
r^j(z,\lambda) = \left[\sum_{j=3}^\infty c_j u^{j-3}
\left(\frac{r(z,\lambda)}{u}\right)^{j-3}\right] \cdot
r^3(z,\lambda)
\end{align*}
holds for any $u \in \CS\backslash\{0\}$. Since, for any $m \geq 3$,
\begin{align*}
\sum_{j=3}^m c_j u^{j-3} \left(\frac{r(z,\lambda)}{u}\right)^{j-3} &
= \sum_{j=3}^{m-1} \left(\sum_{\ell=3}^j c_\ell
u^{\ell-3}\right)\left(1 -
\frac{r(z,\lambda)}{u}\right)\left(\frac{r(z,\lambda)}{u}\right)^{j-3}\\
& \ \ \ + \left(\sum_{\ell = 3}^m c_\ell u^{\ell-3}\right)\cdot
\left(\frac{r(z,\lambda)}{u}\right)^{m-3}
\end{align*}
by Abel transformation, we have
\begin{align*}
\left|\sum_{j=3}^m c_j u^{j-3}
\left(\frac{r(z,\lambda)}{u}\right)^{j-3}\right| & \leq \sup_{3\leq
j \leq m-1} \left\{\left|\sum_{\ell=3}^j c_\ell
u^{\ell-3}\right|\right\} \cdot \left|1 -
\frac{r(z,\lambda)}{u}\right| \cdot \sum_{j=3}^{m-1}
\left|\frac{r(z,\lambda)}{u}\right|^{j-3}\\
& \ \ \ + \left|\frac{S_m(u)}{u^3} \right|\cdot
\left|\frac{r(z,\lambda)}{u}\right|^{m-3}, \ \ \ m >3.
\end{align*}
Then, letting $m \to \infty$ in the above inequality, it follows
that
\begin{align*}
|r(E(z),\lambda)|  & \leq \sup_{m \geq 3} \left\{\left|\sum_{j=3}^m
c_j u^{j-3}\right|\right\} \cdot \left|1 -
\frac{r(z,\lambda)}{u}\right| \cdot
\sum_{m=3}^\infty \left|\frac{r(z,\lambda)}{u}\right|^{m-3}\cdot |r(z,\lambda)|^3 \\
& = \sup_{m \geq 3} \left\{\left|\frac{S_m(u)}{u^3}\right|\right\}
\cdot \frac{\left|1 - \frac{r(z,\lambda)}{u}\right|}{1 -
\left|\frac{r(z,\lambda)}{u}\right|} \cdot |r(z,\lambda)|^3 \\
& = \sup_{m \geq 3} \left\{\left|\frac{S_m(u)}{u^3}\right|\right\}
\cdot \frac{|u| + |r(z,\lambda)|}{|u| - \left|r(z,\lambda)\right|}
\cdot |r(z,\lambda)|^3
\end{align*}
for any $r(z,\lambda) \in \mathcal {D}_0\backslash\{1\}$ and any $u
\in \mathcal {D}_0$ subject to $|u|
> |r(z,\lambda)|$, which verifies (\ref{ineq:abs_r(E(z))_2}). The
proof is completed.
\end{proof}



Based on Lemma \ref{lem:r(E(z))_r(z)}, we have the following lemma
which is more refined than the one obtained in \cite[Corollary
3.1]{Cardoso2011}.


\begin{lemma}
\label{lem:r(z)_convergence1} Let $r(z,\lambda)$ be defined in
$(\ref{fun:r(z)})$. If $r(z_0,\lambda) \in
\overline{\MCD}_1\backslash\{0,1\}$ for some $z_0 \in
\CS\backslash\{0\}$, then the sequence $\{z_k\}$ starting from $z_0$
generated by the scalar case of $(\ref{it:EM})$ for solving
$(\ref{fun:f(z)})$ exists,
\begin{equation}
\label{ineq:abs_r(zk)_1} |r(z_k,\lambda)| \leq q_1^{3^{k-1}}(z_0), \
\ \ k = 1, 2, \ldots,
\end{equation}
where
\begin{equation}
\label{cons:q1(z0)} q_1(z_0) = q_1(z_0,\lambda) :=
\left|\sum_{j=3}^\infty c_j r^{j-1}(z_0,\lambda)\right| < 1,
\end{equation}
and so $|r(z_k,\lambda)| \to 0$ with order $3$ as $k \to \infty$.
\end{lemma}

\begin{proof}
For $z_0$ chosen, $q_1(z_0) < 1$ follows from the same arguments
used in (\ref{ineq:abs_r(E(z))}). By (\ref{ineq:abs_r(E(z))_1}) in
Lemma \ref{lem:r(E(z))_r(z)}, $z_1 = E(z_0)$ exists and
$$
|r(z_1,\lambda)| = q_1(z_0)|r(z_0,\lambda)| \leq q_1(z_0) < 1.
$$
Suppose that $z_k$ exists and (\ref{ineq:abs_r(zk)_1}) holds for
some $k \geq 1$, then by Lemma \ref{lem:r(E(z))_r(z)} again,
$z_{k+1} = E(z_k)$ exists and
\begin{equation*}
|r(z_{k+1},\lambda)| < |r(z_{k},\lambda)|^3  \leq
\left[q_1^{3^{k-1}}(z_0)\right]^3 = q_1^{3^{k}}(z_0).
\end{equation*}
Thus, (\ref{ineq:abs_r(zk)_1}) holds for $k+1$. By induction,
$\{z_k\}$ exists and (\ref{ineq:abs_r(zk)_1}) holds. Furthermore,
$r(z_k,\lambda) \to 0$ with order $3$ as $k \to \infty$, which
completes the proof.
\end{proof}



The following lemma says that, besides in $
\overline{\MCD}_1\backslash\{0,1\}$, it also guarantees
$r(z_k,\lambda)$ converges cubically to 0 as $k \to \infty$ when
$r(z_0,\lambda) \in \MCD_2 \bigcap \MCD_0\backslash\{1\}$ for some
$z_0 \in \CS\backslash\{0\}$.

\begin{lemma}
\label{lem:r(z)_convergence2} 我们   Let $r(z,\lambda)$ be defined
in $(\ref{fun:r(z)})$. For any $z_0 \in \CS\backslash\{0\}$
satisfying $r(z_0,\lambda) \in \MCD_2 \bigcap \MCD_0\backslash\{1\}$
and
\begin{equation}
\label{cons:q2(z0)} q_2(z_0) = q_2(z_0,\lambda) := \sqrt{\sup_{m
\geq 3}
\left\{\frac{|S_m(r(z_0,\lambda))|}{|r(z_0,\lambda)|}\right\} \cdot
\frac{|r(z_0,\lambda)|
+|\phi(r(z_0,\lambda))|}{\big||r(z_0,\lambda)| -
|\phi(r(z_0,\lambda))|\big|}} < 1,
\end{equation}
where $\MCD_2$ is defined in $(\ref{set:D2})$, the sequence
$\{z_k\}$ generated by the scalar form of $(\ref{it:EM})$ starting
from $z_0$ for solving $(\ref{fun:f(z)})$ exists,
\begin{equation}
\label{ineq:abs_r(zk)_2} |r(z_k,\lambda)| \leq q_2^{3^k-1}(z_0)
\cdot |r(z_0,\lambda)|, \ \ \ k = 0, 1, \ldots,
\end{equation}
and so $|r(z_k,\lambda)| \to 0$ with order $3$ as $k \to \infty$.
\end{lemma}

\begin{proof}
For $z_0$ chosen, we have $r(z_0) \in \mathcal
{D}_0\backslash\{1\}$. So, $z_1 = E(z_0)$ exists and $z_1 \neq 0$ by
Lemma \ref{lem:r(E(z))_r(z)}. Recall that $S_m(r(z_0)) \to
\phi(r(z_0))$ as $k\to\infty$ implies
$$
\left|\frac{\phi(r(z_0))}{r(z_0)}\right| \leq \sup_{m\geq3}
\left\{\left|\frac{S_m(r(z_0))}{r(z_0)}\right|\right\} <
q_2^2(z_0)<1,
$$
by (\ref{cons:q2(z0)}), we have
$$
|r(z_1)| = |\phi(r(z_0))| =
\left|\frac{\phi(r(z_0))}{r(z_0)}r(z_0)\right| <
q_2^2(z_0)\cdot|r(z_0)|,
$$
and (\ref{ineq:abs_r(zk)_2}) holds for $k=1$.  Assume $z_0, z_1,
\ldots, z_k$ exist and satisfy (\ref{ineq:abs_r(zk)_2}). Then
$$
|r(z_k)| \leq q_2^2(z_0)|r(z_0)|< |r(z_0)| < \frac{p}{p-1}
(\sqrt{2p-1}-1).
$$
So, by Lemma \ref{lem:r(E(z))_r(z)} with $u = r(z_0)$, $z_{k+1} =
E(z_k)$ exists, $z_{k+1} \neq 0$ and
\begin{align*}
|r(z_{k+1})| & = |\phi(r(z_k))| \\
& \leq \sup_{m \geq 3}
\left\{\frac{|S_m(r(z_0))|}{|r(z_0)|^3}\right\}\cdot \frac{|r(z_0)|
+|\phi(r(z_0))|}{\big||r(z_0)| -
|\phi(r(z_0))|\big|} \cdot |r(z_k)|^3 \\
& \leq \sup_{m \geq 3}
\left\{\frac{|S_m(r(z_0))|}{|r(z_0)|^3}\right\}\cdot \frac{|r(z_0)|
+|\phi(r(z_0))|}{\big||r(z_0)| - |\phi(r(z_0))|\big|}
\left[q_2^{3^k-1}(z_0)\right]^3\cdot|r(z_0)|^3 \\
& = \sup_{m \geq 3}
\left\{\frac{|S_m(r(z_0))|}{|r(z_0)|}\right\}\cdot \frac{|r(z_0)|
+|\phi(r(z_0))|}{\big||r(z_0)| - |\phi(r(z_0))|\big|} \cdot
[q_2(z_0)]^{3^{k+1}-3} \cdot
|r(z_0)| \\
& = [q_2(z_0)]^{3^{k+1}-1} \cdot |r(z_0)|,
\end{align*}
which shows (\ref{ineq:abs_r(zk)_2}) by induction. The proof is
completed.
\end{proof}



Now, based on the above lemmas, we can obtain the following
convergence results for scalar Euler's method (\ref{it:EM}). Define
\begin{equation}
\label{set:R1} \MCR_1 := \big\{\lambda \in \CS: r(z_0,\lambda) \in
\overline{\MCD}_1 \text{ for some } z_0 \in
\CS\backslash\{0\}\big\},
\end{equation}
and
\begin{equation}
\label{set:R2} \MCR_2 := \left\{\lambda \in \CS: r(z_0,\lambda) \in
\MCD_0 \bigcap \MCD_2 \text{ for some } z_0 \in
\CS\backslash\{0\}\right\},
\end{equation}
where $\overline{\MCD}_1$ is the closure of $\MCD_1$ defined in
(\ref{set:D1}), $\mathcal {D}_0$ and $\mathcal {D}_2$ are defined in
(\ref{set:D0}) and (\ref{set:D2}), respectively.



\begin{lemma}
\label{lem:ScalarEMCon1} For any $\lambda \in \MCR_1 \bigcup
\MCR_2$, where $\MCR_1$ and $\MCR_2$ are defined by $(\ref{set:R1})$
and $(\ref{set:R2})$, respectively, the sequence $\{z_k(\lambda)\}$
generated by scalar Euler iteration $(\ref{it:EM})$ with some $z_0
\in \CS\backslash\{0\}$ for solving $(\ref{fun:f(z)})$ converges to
the principal $p$th root $\lambda^{1/p}$. Moreover, if $\lambda \neq
0$, then the convergence order is $3$.
\end{lemma}


\begin{proof}
We prove this lemma by four steps as follows.

Step 1. Suppose $\MCR_c$ is any closed domain in $\MCR_1$ or
$\MCR_2$ and that $0 \not\in \MCR_c$. We will prove in this step
$\{z_k(\lambda)\}$ converges uniformly to $z(\lambda)$, a $p$th root
of each $\lambda \in \MCR_c$, and that $z(\lambda)$ exists for each
$\lambda \in \MCR_1\bigcup\MCR_2$.

We write $r(z_k,\lambda) \triangleq r(z_k(\lambda),\lambda)$ for
short below. Since $ \sum\limits_{j=3}^\infty c_j r^j(z_0,\lambda)$
is analytic for each $\lambda \in \MCR_1\bigcup\MCR_2$ and $\MCR_c
\subset \MCR_1\bigcup\MCR_2$ is bounded, there is a
$\widehat{\lambda} \in
\partial\MCR_c$ such that
\begin{equation}
\label{eq:max_mod_1} \left|\sum_{j=3}^\infty c_j
r^j(z_0,\widehat{\lambda})\right| = \max_{\lambda \in
\MCR_c}\left|\sum_{j=3}^\infty c_j r^j(z_0,\lambda)\right|
\end{equation}
by the theorem of maximum modulus of analytic function. Let
$$
q(z_0) := \left\{
\begin{array}{ll}
\displaystyle \max_{\lambda \in \MCR_c}q_1(z_0,\lambda), & \mbox{if
$\MCR_c
\subset \MCR_1$,} \\
\displaystyle \max_{\lambda \in \MCR_c}q_2(z_0,\lambda), & \mbox{if
$\MCR_c \subset \MCR_2$},
\end{array}
\right.
$$
where $q_1(z_0,\lambda)$ and $q_2(z_0,\lambda)$ are defined in
(\ref{cons:q1(z0)}) and (\ref{cons:q2(z0)}), respectively. Then
$$
q(z_0) = \left\{
\begin{array}{ll}
q_1(z_0,\widehat{\lambda}) < 1, & \mbox{if $\MCR_c
\subset \MCR_1$,} \\
q_2(z_0,\widehat{\lambda}) < 1, & \mbox{if $\MCR_c \subset \MCR_2$},
\end{array}
\right.
$$
and
\begin{equation}
\label{ineq:abs_r(zk)} |r(z_k, \lambda)| \leq \left\{
\begin{array}{ll}
q^{3^{k-1}}(z_0), & \mbox{if $\MCR_c
\subset \MCR_1$,} \\
q^{3^k-1}(z_0) \cdot r_*, & \mbox{if $\MCR_c \subset \MCR_2$},
\end{array} \ \ \ k = 1,2,\ldots, \lambda \in \MCR_c
\right.
\end{equation}
by (\ref{eq:max_mod_1}), Lemmas \ref{lem:r(z)_convergence1} and
\ref{lem:r(z)_convergence2}, where $r_* := \max\limits_{\lambda \in
\MCR_c}|r(z_0,\lambda)|$ is a positive real independent on $\lambda
\in \MCR_c$. It follows $\{r(z_k, \lambda)\}$ converges uniformly to
0 with order 3 as $k \to \infty$ for all $\lambda \in \MCR_c$ and
that identities
\begin{equation}
\label{eq:zkp} z_k^p = \frac{\lambda}{1 - r(z_k,\lambda)}, \ \ \
\lambda \in \MCR_c, \ k = 0, 1, \ldots
\end{equation}
give the sequence $\{z_k(\lambda)\}$ is bounded uniformly for all
$\lambda \in \MCR_c$. Thus, there is a $M > 0$, independent on $k$
and $\lambda \in \MCR_c$, such that
\begin{equation}
\label{ineq:abs_bounded_M} \frac{1}{2p^2} |z_k(\lambda)| |2p + (p -
1)r(z_k, \lambda)| \leq M, \ \ k \geq 0, \lambda \in \MCR_c.
\end{equation}
By (\ref{it:EM_fun}) and (\ref{ineq:abs_bounded_M}),
\begin{align*}
|z_{k + 1}(\lambda) - z_k(\lambda)| & = \left|
\frac{1}{2p^2}z_k(\lambda) \left[(3p - 1) - 2(2p
- 1)\lambda z_k^{-p}(\lambda) + (p - 1)\lambda^2 z_k^{-2p}(\lambda)\right]\right| \\
& = \left| \frac{1}{2p^2}z_k(\lambda) \left[(1 - p) + 2(2p - 1)
r(z_k, \lambda) + (p -
1) (1 - r(z_k, \lambda))^2\right]\right| \\
& = \left| \frac{1}{2p^2}z_k(\lambda) \left[ 2p r(z_k, \lambda) + (p
-
1)r(z_k, \lambda)^2\right] \right| \\
& = \frac{1}{2p^2} |z_k(\lambda)| |r(z_k, \lambda)| |2p + (p - 1)r(z_k, \lambda)| \\
& \leq M |r(z_k, \lambda)|, \ \ \ \lambda \in \MCR_c, \ k =0, 1,
\ldots,
\end{align*}
which and (\ref{ineq:abs_r(zk)}) conclude that $\{z_{k + 1}(\lambda)
- z_k(\lambda)\}$ is majoriant by a geometrical sequence which
converges to zero and independent on $\lambda \in \MCR_c$. So,
$\{z_k(\lambda)\}$ is a Cauchy sequence for each $\lambda \in
\MCR_c$ and there is $z(\lambda)$ defined on $\MCR_c$ such that
$z_k(\lambda)$ converges uniformly to $z(\lambda)$ for all $\lambda
\in \MCR_c$. Let $k \to \infty$ in (\ref{eq:zkp}), we have
$$
z^p(\lambda) = \lambda, \ \ \ \lambda \in \MCR_c.
$$
That is, $z(\lambda)$ is a $p$th root of $\lambda \in \MCR_c$.


Step 2. Since for any $\lambda \in \MCR_1 \bigcup \MCR_2$, there is
a closed domain of $\MCR_1$ or $\MCR_2$ such that $\lambda$ belongs
to it. By Step 1, $z(\lambda)$ exists for each $\lambda \in
\MCR_1\bigcup \MCR_2$. In this step, we will show $z(\lambda)$
obtained in Step 1 is analytic in $\Int(\MCR_1)$, the interior of
$\MCR_1$, and $\MCR_2$. Therefore, $z(\lambda)$ located in a
single-valued branch of root function in $\Int(\MCR_1)$ and
$\MCR_2$.

In fact, by the definition of $z_k(\lambda), \lambda \in
\MCR_1\bigcup\MCR_2$, we have $z_k(\lambda)$ is analytic in
$\Int(\MCR_1)$ or $\MCR_2$. Since $\{z_k(\lambda)\}$ converges
uniformly to $z(\lambda)$ by Step 1, $z_k(\lambda)$ is analytic in
$\Int(\MCR_1)$ and $\MCR_2$ by Weierstrass theorem, seperately.
Recall that $z(\lambda)$ is a $p$th root of $\lambda \in
\Int(\MCR_1)$ or $\MCR_2$, we get $z(\lambda)$ located in a
single-valued branch of root function for $\lambda \in \Int(\MCR_1)$
or $\lambda \in \MCR_2$, seperately.


Step 3. In this step, we will show that $z(\lambda) \to
z(\lambda_0)$ as $\lambda \to \lambda_0$ from the inner side of
$\MCR_1$, where $\lambda_0 \in \partial\MCR_1$ and $\lambda_0 \neq
0$.

It is clear that $|r(z_0, \lambda_0)| < 1$ for any $\lambda_0 \in
\partial\MCR_1$ and $\lambda_0 \neq 0$. Then, there is $\delta_0 >
0$ such that closed domain $\overline{O}(\lambda_0, \delta_0)
\bigcap \MCR_1$ does not contains 0 and 1, where
$\overline{O}(\lambda_0, \delta_0) := \{\lambda \in \CS: |\lambda -
\lambda_0| \leq \delta_0\}$. By Step 1, $\{z_k(\lambda)\}$ converges
uniformly to $z(\lambda)$ for any $\lambda$ in
$\overline{O}(\lambda_0, \delta_0) \bigcap \MCR_1$. So, for any
$\varepsilon > 0$, there exists $K > 0$ such that for all $k \geq K$
and $\lambda \in \overline{O}(\lambda_0, \delta_0) \bigcap \MCR_1$,
\begin{equation}
\label{ineq:abs_zk-z_1} |z_k(\lambda) - z(\lambda)| <
\frac{\varepsilon}{3}.
\end{equation}
Since $z_k(\lambda)$ is analytic in $\MCR_1$ derives $z_k(\lambda)$
is continuous in $\overline{O}(\lambda_0, \delta_0) \bigcap \MCR_1$,
there exists $0 < \delta_1 < \delta_0$ such that
$\overline{O}(\lambda_0, \delta_1) \subset \overline{O}(\lambda_0,
\delta_0)$ and
\begin{equation}
\label{ineq:abs_zk-z_2} |z_k(\lambda) - z_k(\lambda_0)| <
\frac{\varepsilon}{3}, \ \ \ \forall \ \lambda \in
\overline{O}(\lambda_0, \delta_1)\bigcap \MCR_1.
\end{equation}
Thus, for all $\lambda$ in $\overline{O}(\lambda_0, \delta_1)\bigcap
\MCR_1$, we have
$$
|z(\lambda) - z(\lambda_0)| \leq |z(\lambda) - z_k(\lambda)| +
|z_k(\lambda) - z_k(\lambda_0)| + |z_k(\lambda_0) - z(\lambda_0)| <
\frac{\varepsilon}{3} + \frac{\varepsilon}{3} +
\frac{\varepsilon}{3} = \varepsilon
$$
by (\ref{ineq:abs_zk-z_1}) and (\ref{ineq:abs_zk-z_2}). That is,
$z(\lambda)$ is continuous at $\lambda = \lambda_0$. The
arbitrariness of $\lambda_0$ completes the proof of Step 3.


Step 4. This is the last step of the proof.

By Step 2, $z(\lambda)$ is analytic in $\Int(\MCR_1)$ and it is
located in a single-valued branch of $p$th root function. Since
$z_k(1) \equiv 1$ for all $k$ implies $z(1) = 1$, we have that
$z(\lambda)$ located in the single-valued branch containing 1. That
is, $z(\lambda)$ is the principal $p$th root of each $\lambda$ in
$\Int(\MCR_1)$. Since $z(\lambda) \to z(\lambda_0)$ as $\lambda \to
\lambda_0 \ (\lambda \in \Int(\MCR_1))$ for each $\lambda_0 \in
\partial\MCR_1$ by Step 3, we get $z(\lambda_0)$ is also the principal $p$th
root of $\lambda_0 \in \partial\MCR_1$.

By now, we have proved that $z(\lambda)$ is the principal $p$th root
of each $\lambda \in \MCR_1$ except $\lambda = 0$. When $\lambda =
0$, we have by (\ref{it:EM}) that
$$
z_k(0) = \frac{2p^2 - 3p + 1}{2p^2} z_{k-1}(0) = \left(\frac{2p^2 -
3p + 1}{2p^2}\right)^k z_0, \ \ k = 0, 1, 2, \ldots.
$$
So, $\{z_k(0)\}$ converges to 0, the principal $p$th root of 0,
linearly. Therefore, $z(\lambda)$ is the principal $p$th root of
each $\lambda \in \MCR_1$.

By Step 2 again, $z(\lambda)$ is analytic and locates in a
single-valued branch of $p$th root function for each $\lambda \in
\MCR_2$. Now, we can see that, if we can show $\MCR_2$ contains some
part of $\partial\MCR_1$, then the single-valued branch of
$z(\lambda)$ for $\lambda \in \MCR_2$ is the same branch of
$z(\lambda)$ for $\lambda \in \MCR_1$, which deduces that
$z(\lambda)$ is the principle $p$th root of $\lambda \in \MCR_1
\bigcup\MCR_2$. So, what we need to do is to prove $\MCR_2$ contains
some part of $\partial\MCR_1$.

Set $\lambda_0 = z_0^p$ and define
$$
S_m(z_0,\lambda) := \sum\limits_{j=3}^m c_jr^{j-1}(z_0,\lambda), \ \
\ S_\infty(z_0,\lambda) := \sum\limits_{j=3}^\infty
c_jr^{j-1}(z_0,\lambda), \ \ \ m \geq 3, \lambda \in \partial\MCR_1.
$$
Clearly, $S_\infty(z_0,\lambda)$ is continuous with respect to
$\lambda$ on $\partial\MCR_1$ and
\begin{equation}
\label{ineq:S(z0,lambda0)} 0 = \left|S_\infty(z_0,\lambda_0)\right|
= \min_{\lambda \in
\partial\MCR_1}\left|S_\infty(z_0,\lambda)\right| \leq
\left|S_\infty(z_0,\lambda)\right| \leq 1, \ \ \ \forall \ \lambda
\in
\partial\MCR_1.
\end{equation}
Since $|S_m(z_0,\lambda)|<1$ holds for all $\lambda \in
\MCR_1\bigcup\MCR_2$, we can choose $M < 1$ be a real satisfying
$$
\sup_{m\geq3}\left|S_m(z_0,\lambda)\right| < M, \ \ \ \lambda \in
\partial\MCR_1.
$$
By (\ref{ineq:S(z0,lambda0)}), there is $\delta > 0$ such that once
$|\lambda - \lambda_0| < \delta$, then
$$
 \left|S_\infty(z_0,\lambda)\right| <
 \frac{\frac{1}{M}-1}{\frac{1}{M} + 1},
$$
or equivalently,
$$
q_2(z_0,\lambda) = \sup_{m\geq 3}|S_m(z_0,\lambda)|\cdot
\frac{1+|S_\infty(z_0,\lambda)|}{1-|S_\infty(z_0,\lambda)|} < M
\cdot \frac{1}{M} = 1.
$$
Therefore, $\MCR_2$ contains some part of $\partial\MCR_1$. The
proof is completed.
\end{proof}



Choosing $z_0 \equiv 1$ in $\MCR_1$ and $\MCR_2$ given by
(\ref{set:R1}) and (\ref{set:R2}), respectively, one has that $\MCR
= \MCR_1 \bigcup \MCR_2$. So, we obtain from Lemma
\ref{lem:ScalarEMCon1} the following corollary:

\begin{corollary}
\label{cor:zk_convergence1} For any $\lambda \in \MCR$ defined in
$(\ref{set:R})$, the sequence $\{z_k(\lambda)\}$ generated by scalar
Euler iteration $(\ref{it:EM})$ with $z_0 = 1$ for solving
$(\ref{fun:f(z)})$ converges to the principal $p$th root
$\lambda^{1/p}$. Moreover, if $\lambda \neq 0$, then the convergence
order is $3$.
\end{corollary}



Next, we consider the case of matrix form. For the matrix $A \in
\CS^{n \times n}$ defined in (\ref{eq:f(X)=0}), let
\begin{equation}
\label{fun:R(X)} R(X) := \I - A X^{-p}, \ \ \ p \geq 2
\end{equation}
for any nonsingular matrix $ X \in \CS^{n \times n}$.

\begin{lemma}
\label{lem:R(E(X))_R(X)} Let $X \in \CS^{n\times n}$ be a
nonsingular matrix commuting with $A$ and $R(X)$ be defined in
$(\ref{fun:R(X)})$. If the spectrum $\sigma(R(X)) \subset \mathcal
{D}_0$ for some nonsingular matrix $X \in \CS^{n \times n}$, where
$\mathcal {D}_0$ is defined in $(\ref{set:D0})$, then $E(X)$
generated by $(\ref{it:EM_fun})$ is nonsingular, commutes with $A$
and
\begin{equation}
\label{eq:r(E(X))} R(E(X)) = \I - \left[\I - \frac{1}{p} R(X) -
\frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)\right]^{-p}\cdot(\I - R(X)) = \phi(R(X)).
\end{equation}
\end{lemma}

\begin{proof}
Clearly, $E(X)$ given by (\ref{it:EM_fun}) exists for any
nonsingular matrix $X \in \CS^{n \times n}$. Since $\sigma(R(X))
\subset \mathcal {D}_0\backslash\{0\}$, we get
\begin{align*}
\rho\left(\frac{1}{p} R(X) + \frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)\right) & \leq \frac{1}{p}\cdot\rho(R(X))
+
\frac{1}{2}\left(\frac{1}{p} - \frac{1}{p^2}\right)\rho^2(R(X)) \\
& = \frac{1}{p} \cdot
\rho(R(X)) \cdot \left(1 + \frac{p-1}{2p}\rho(R(X))\right) \\
& < \frac{1}{p} \cdot \frac{p}{p-1} \left(\sqrt{2p-1} - 1\right)
\cdot \left(1 + \frac{\sqrt{2p-1} - 1}{2} \right) \\
& = 1.
\end{align*}
Here, the first inequality follows from $\frac{1}{p} R(X) +
\frac{1}{2}(\frac{1}{p} - \frac{1}{p^2})R^2(X)$ is a polynomial of
the matrix $R(X)$. So
$$
\I - \frac{1}{p} R(X) - \frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)
$$
is nonsingular by Neumann Lemma. It follows that
\begin{align*}
E(X) & = \frac{1}{2p^2} X \left[(2 p^2 - 3 p + 1)\I + 2(2 p - 1)
AX^{-p} - (p - 1)(AX)^2\right] \\
& = X \left[\frac{2p^2 - 3p + 1}{2p^2}\I + \frac{2p -
1}{p^2}(\I - R(X)) - \frac{p - 1}{2p^2}(1 - R(X))^2\right] \\
& = X \left[\I - \frac{1}{p} R(X) - \frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)\right]
\end{align*}
is also nonsingular and commutes with $A$, and
\begin{equation*}
R(E(X)) = \I - \left[\I - \frac{1}{p} R(X) -
\frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)\right]^{-p}\cdot(\I - R(X)) = \phi(R(X))
\end{equation*}
by the assumption of $X$ commutes with $A$. This completes the
proof.
\end{proof}



Thanks to Lemma \ref{lem:R(E(X))_R(X)}, we have

\begin{corollary}
\label{cor:E(Xk)} If $X_0 \in \CS^{n \times n}$ commutes with $A$
and  the spectrum $\sigma(R(X_0)) \subset \mathcal {D}_0$, where
$\mathcal {D}_0$ is defined in $(\ref{set:D0})$, then the sequence
$\{X_k\}$ starting from $X_0$ generated by Euler's method
$(\ref{it:EM})$ for solving $(\ref{eq:f(X)=0})$ exists.
\end{corollary}



\begin{lemma}
\label{lem:norm_R(E(X))_R(X)} If the spectrum $\sigma(R(X)) \subset
\MCD_1\backslash\{0\}$ for some nonsingular matrix $X \in \CS^{n
\times n}$, where $R(X)$  be defined in $(\ref{fun:R(X)})$ and
$\MCD_1$ is defined in $(\ref{set:D1})$, then, there is a
sub-multiplicative matrix norm $\|\cdot\|$ such that $\|R(X)\| \leq
1$ and
\begin{equation}
\label{ineq:norm_R(E(X))_1} \|R(E(X))\| \leq
\frac{\phi(\|R(X)\|)}{\|R(X)\|^3} \cdot \|R(X)\|^3 < \|R(X)\|^3,
\end{equation}
where $\phi$ is defined by $(\ref{fun:phi(z)})$.
\end{lemma}

\begin{proof}
It follows from Lemma \ref{lem:R(E(X))_R(X)} that $R(E(X))$ exists.
Since the spectrum $\sigma(R(X)) \subset \mathcal {D}_1$, the
spectral radius of $R(X)$ is less than 1. So, there is a
sub-multiplicative matrix norm $\|\cdot\|$ such that $\|R(X)\| < 1$.
Note that, for any $u \in (0,1)$,  It follows from Lemma
\ref{lem:RatFunTayExp} that
$$
\frac{\phi(u)}{u^3} = \sum_{j=3}^\infty c_j u^{j-3} <
\sum_{j=3}^\infty c_j = 1.
$$
Thus, this together with (\ref{eq:r(E(X))}) gives that
\begin{equation*}
\|R(E(X))\|  = \|\phi(R(X))\| \leq \phi(\|R(X)\|) =
\frac{\phi(\|R(X)\|)}{\|R(X)\|^3}\|R(X)\|^3 < \|R(X)\|^3,
\end{equation*}
which shows (\ref{ineq:norm_R(E(X))_1}). The proof is completed.
\end{proof}



\begin{lemma}
\label{lem:R(X)_convergence1} Let $R(X)$  be defined in
$(\ref{fun:R(X)})$. Suppose the spectrum $\sigma(R(X_0)) \subset
\mathcal {D}_1\backslash\{0\}$ for some nonsingular matrix $X_0 \in
\CS^{n \times n}$ and that $\|R(X_0)\| < 1$ for a sub-multiplicative
matrix norm $\|\cdot\|$, where $\mathcal {D}_1$ is defined in
$(\ref{set:D1})$. Let $\{X_k\}$ be the sequence  starting from $X_0$
generated by Euler's method $(\ref{it:EM})$ for solving
$(\ref{eq:f(X)=0})$. Then we have
\begin{equation}
\label{ineq:norm_R(Xk)_1} \|R(X_k)\| \leq q^{3^k-1}(X_0) \cdot
\|R(X_0)\|, \ \ \ k = 1, 2, \ldots,
\end{equation}
where
\begin{equation}
\label{cons:q(X0)} q(X_0) :=
\sqrt{\frac{\phi(\|R(X_0)\|)}{\|R(X_0)\|}} < 1
\end{equation}
and $\phi$ is defined by $(\ref{fun:phi(z)})$.
\end{lemma}

\begin{proof}
For $X_0$ chosen, by (\ref{ineq:norm_R(E(X))_1}) in Lemma
\ref{lem:norm_R(E(X))_R(X)}, we have
$$
\|R(X_1)\| \leq \frac{\phi(\|R(X_0)\|)}{\|R(X_0)\|^3} \|R(X_0)\|^3 =
q^2(X_0) \cdot \|R(X_0)\|.
$$
If (\ref{ineq:norm_R(Xk)_1}) holds for some $k \geq 1$, then by
Lemma \ref{lem:norm_R(E(X))_R(X)} again, one has that
\begin{align*}
\|R(X_{k+1})\| & \leq
\frac{\phi(\|R(X_k)\|)}{\|R(X_k)\|^3}\|R(X_k)\|^3\\
& \leq \frac{\phi(\|R(X_0)\|)}{\|R(X_0)\|^3}
\left[q^{3^k-1}(X_0)\right]^3 \cdot \|R(X_0)\|^3\\
& = [q(X_0)]^{3^{k+1}-1}\cdot\|R(X_0)\|.
\end{align*}
Thus, by induction, (\ref{ineq:norm_R(Xk)_1}) holds for all $k \geq
1$. This completes the proof.
\end{proof}



\begin{lemma}
\label{lem:R(X)_convergence2} Let $R(X)$  be defined in
$(\ref{fun:R(X)})$. If the spectrum $\sigma(R(X_0)) \subset \mathcal
{D}_2 \bigcap \mathcal {D}_0\backslash\{0\}$ for some nonsingular
matrix $X_0 \in \CS^{n \times n}$, where $\mathcal {D}_0$ and
$\mathcal {D}_2$ are defined in $(\ref{set:D0})$ and
$(\ref{set:D2})$, respectively. Let $\{X_k\}$ be the sequence
starting from $X_0$ generated by Euler's method $(\ref{it:EM})$ for
solving $(\ref{eq:f(X)=0})$. Then, there exists $\widehat{N} > 0$
such that
\begin{equation}
\label{ineq:norm_R(Xk)_2} \|R(X_k)\| \leq
[q(X_{\widehat{N}})]^{3^{k-\widehat{N}}-1} \cdot
\|R(X_{\widehat{N}})\|, \ \ \ \forall \ k
> \widehat{N},
\end{equation}
where $q(X_{\widehat{N}})$ is defined as $q(X_0)$ in
$(\ref{cons:q(X0)})$ by substituting $X_{\widehat{N}}$ for $X_0$.
\end{lemma}

\begin{proof}
For any $r(z_0) \in \sigma(R(X_0))$, let $\{z_k\}$ be the sequence
 generated by the scalar form of (\ref{it:EM}) starting from $z_0$. It
follows from Lemma \ref{lem:r(z)_convergence2} that there exists $N
> 0$ such that $|r(z_N)| < 1$. Then, we can get from Lemma
\ref{lem:r(z)_convergence1} that
\begin{equation*}
\label{ineq:abs_r(zk)_3} |r(z_k)| < |r(z_N)|^{3^{k-N}}, \ \ \
\forall \ k > N.
\end{equation*}
Define
$$
\widehat{N} := \max_{r(z_0) \in \sigma(R(X_0))} \{N: \text{choose a
}N
> 0 \text{ such that } |r(z_N)| < 1\}.
$$
Then, there exists a sub-multiplicative matrix norm $\|\cdot\|$ such
that $\|X_{\widehat{N}}\| < 1$. Thus, (\ref{ineq:norm_R(Xk)_2})
follows from Lemma \ref{lem:R(X)_convergence1}. This completes the
proof.
\end{proof}



\subsection{Proof of Theorem \ref{th:MatrixEMCon}}



The following lemma is taken from \cite[Theorem 4.15]{Higham2008},
which allows us to deduce convergence of the matrix iteration
sequence generated by Euler's method (\ref{it:EM}).

\begin{lemma}[\cite{Higham2008}]
\label{lem:MatIteConLem} Suppose that $g(x,t)$ is a rational
function with respect to its two variables and that $x^* =
f(\lambda)$ is an attracting fixed point of the iteration $x_{k + 1}
= g(x_k, \lambda), x_0 = \phi_0(\lambda)$, where $\phi_0$ is a
rational function and $\lambda \in \CS$. Then, the matrix sequence
generated by $X_{k + 1} = g(X_k, J(\lambda)), X_0 =
\phi_0(J(\lambda))$, converges to a matrix $X^*$ with $(X^*)_{ii}
\equiv f(\lambda)$, $i = 1, 2, \ldots, m$, where $J(\lambda) \in
\CS^{m \times m}$ is a Jordan block.
\end{lemma}


Now, we can prove Theorem \ref{th:MatrixEMCon} by applying the above
lemma together with the lemmas proposed in Section 3.1.



\begin{proof}[The proof of Theorem $\ref{th:MatrixEMCon}$]
By Corollary \ref{cor:E(Xk)}, the matrix sequence $\{X_k\}$
generated by Euler's method (\ref{it:EM}) starting from $X_0 = \I$
is well defined when the eigenvalues of $A$ are in $\mathcal {R}
\subset \mathcal {D}_0$. Thanks to Lemma \ref{lem:MatIteConLem},
$\{X_k\}$ converges to the principal $p$th root of $A$ follows from
Corollary \ref{cor:zk_convergence1}. The first part of theorem is
completed.

For the second part, let $X_* = A^{1/p}$ and $E_k = X_k - X_*$ for
$k \geq 0$. Due to the commutativity of $X_k$ and $X_*$, we have
\begin{align}
R(X_k) & = \I - AX_k^{-p} = (X_k^p - X_*^p)X_k^{-p} \nonumber\\
& = (X_k - X_*)\left(X_k^{p - 1} + X_k^{p - 2}X_* + \cdots +
X_k X_*^{p - 2} + X_*^{p - 1}\right)X_k^{-p} \nonumber\\
& = E_k \left(X_k^{p - 1} + X_k^{p - 2}X_* + \cdots + X_k X_*^{p -
2} + X_*^{p - 1}\right) X_k^{-p}, \ \ \forall \ k \geq 0
\label{eq:R(Xk)}
\end{align}
Set
$$
Y_k := \sum_{i = 1}^p X_k^{p - i} X_*^{i - 1} = X_k^{p - 1} + X_k^{p
- 2}X_* + \cdots + X_k X_*^{p - 2} + X_*^{p - 1}.
$$
Since $X_k$ converges to $A^{1/p}$ and all the eigenvalues of
$A^{1/p}$ are not in $\RS^-$, there exists nonnegative integer $N
> 0$ such that the eigenvalues $X_k$ are not in $\RS^-$ for all $k
\geq N$. Thus, the eigenvalues of $Y_k$ are also not in $\RS^-$ and
so $Y_k$ is nonsingular for $k \geq N$. Then, it follows from
(\ref{eq:R(Xk)}) that
\begin{equation}
\label{eq:Ek+1} E_{k + 1} = R(X_{k + 1}) X_{k + 1}^p Y_{k + 1}^{-1},
\ \ k \geq N.
\end{equation}
Thanks to (\ref{ineq:norm_R(Xk)_1}) and (\ref{ineq:norm_R(Xk)_2}) in
Lemmas \ref{lem:R(X)_convergence1} and \ref{lem:R(X)_convergence2},
respectively, there exists  $K_0 > 0$ such that $\|R(X_{k + 1})\| <
\|R(X_{k})\|^3$ for any $k \geq K_0$. It follows from
(\ref{eq:R(Xk)}) and (\ref{eq:Ek+1}) that
\begin{align}
\|E_{k + 1}\| & \leq \|R(X_{k + 1})\| \|X_{k + 1}\|^p \|Y_{k + 1}^{-1}\| \nonumber\\
& <  \|R(X_k)\|^3 \|X_{k + 1}\|^p \|Y_{k + 1}^{-1}\| \nonumber\\
& \leq \left(\|X_k^{-1}\|^p\|X_{k + 1}\|^p \|Y_k\|\|Y_{k +
1}^{-1}\|\right) \|E_k\|^3, \ \ \forall \ k \geq K_0.
\label{ineq_norm_Ek+1}
\end{align}
Thus $\{X_k\}$ is convergent guarantees that $\|X_k^{-1}\|^p\|X_{k +
1}\|^p \|Y_k\|\|Y_{k + 1}^{-1}\|$ is bounded for all $k \geq K_0$.
(\ref{ineq_norm_Ek+1}) concludes that the local convergence order is
3. This completes the proof.
\end{proof}




\section{A Preconditioned Schur Modification}


Let's begin with considering the stability of Euler iteration
(\ref{it:EM}). The analysis approach here follows the line of
stability analysis for Newton iteration in \cite{Smith2003}.

Let $A$ be nonsingular and diagonalizable with $n$ different
eigenvalues $\lambda_1, \ldots, \lambda_n$. Then, there exists a
nonsingular matrix $Z$ such that
$$
\inv{Z} A Z = \Lambda := \diag(\lambda_1, \ldots, \lambda_n).
$$
Suppose that the sequence $\{X_k\}$ generated by Euler iteration
(\ref{it:EM}) starting from $X_0 = \I$ converges to the principle
$p$th root $A^{1/p}$. Set $D_k := \inv{Z} X_k Z, \ k \geq 0$. It
follows from (\ref{it:EM}) that
$$
D_{k + 1} = \frac{1}{2p^2} D_k \left[(2p^2 - 3p + 1)\I + 2(2p -
1)\Lambda D_k^{-p} - (p - 1)\left(\Lambda D_k^{-p}\right)^2\right],
\ \ k \geq 0.
$$
Since $D_0 = \inv{Z} X_0 Z = \I$, $D_k$ is diagonal for any $k \geq
1$. Thus, if we set $D_k := \diag(d_1^{(k)}, \ldots, d_n^{(k)})$,
then $d_j^{(k)}$ converges to $\lambda_j^{1/p}$ as $k \to \infty$.
Let $\{\widetilde{X}_k\}$ be the sequence of computed iterates, then
\begin{align}
\widetilde{X}_{k + 1} & = \frac{1}{2p^2} \left((2p^2 - 3p +
1)\widetilde{X}_k + 2(2p - 1) A\widetilde{X}_k^{1-p} - (p - 1)A^2
\widetilde{X}_k^{1 -
2p}\right) \nonumber \\
& = \frac{2p^2 - 3p + 1}{2p^2}(X_k + \Delta_k) + \frac{2p - 1}{p^2}
A (X_k + \Delta_k)^{1 - p} - \frac{p - 1}{2p^2}A^2 (X_k +
\Delta_k)^{1 - 2p}.\label{eq:wide_Xk+1}
\end{align}
Set $\Delta_k = \widetilde{X}_k - X_k$ and $\widetilde{\Delta}_k :=
(\widetilde{\delta}_{ij}^{(k)}) = \inv{Z} \Delta_k Z$. Applying the
following expansion \cite{Smith2003}
\begin{align*}
(X + \Delta_X)^{1 - p} = X^{1-p} - \sum_{\ell = 1}^{p - 1} X^{\ell -
p} \Delta_X X^{-\ell} + O(\|\Delta_X\|^2)
\end{align*}
to (\ref{eq:wide_Xk+1}), where $\|\cdot\|$ denotes the 2-norm, one
has that
\begin{align*}
\widetilde{X}_{k + 1} & = \frac{2p^2 - 3p + 1}{2p^2}(X_k + \Delta_k)
+ \frac{2p - 1}{p^2} A \left[X_k^{1-p} - \sum_{\ell = 1}^{p - 1}
X_k^{\ell
- p} \Delta_k X_k^{-\ell}\right] \\
& \qquad - \frac{p - 1}{2p^2}A^2 \left[X_k^{1-2p} - \sum_{\ell =
1}^{2p - 1} X_k^{\ell - 2p} \Delta_k X_k^{-\ell}\right] +
O(\|\Delta_k\|^2), \ \ k \geq 0.
\end{align*}
We have for each $k \geq 0$,
\begin{align*}
\Delta_{k + 1} & = \widetilde{X}_{k + 1} - X_{k + 1} \nonumber\\
& = \frac{2p^2 - 3p + 1}{2p^2} \Delta_k - \frac{2p - 1}{p^2} A
\sum_{\ell = 1}^{p - 1} X_k^{\ell - p} \Delta_k X_k^{-\ell} +
\frac{p - 1}{2p^2} A^2 \sum_{\ell = 1}^{2p - 1} X_k^{\ell - 2p}
\Delta_k X_k^{-\ell},% \label{eq:Deltak+1}
\end{align*}
and
\begin{align*}
\widetilde{\Delta}_{k + 1} & = \frac{2p^2 - 3p + 1}{2p^2}
\widetilde{\Delta}_k - \frac{2p - 1}{p^2} \Lambda \sum_{\ell = 1}^{p
-
1} D_k^{\ell - p} \widetilde{\Delta}_k D_k^{-\ell} \\
& \qquad + \frac{p - 1}{2p^2} \Lambda^2 \sum_{\ell = 1}^{2p - 1}
D_k^{\ell - 2p} \widetilde{\Delta}_k D_k^{-\ell} +
O(\|\Delta_k\|^2).
\end{align*}
This gives
\begin{align}
\label{eq:wide_deltak+1}\widetilde{\delta}_{ij}^{(k + 1)}  & =
\widetilde{\delta}_{ij}^{(k)} \pi_{ij}^{(k)} + O(\|\Delta_k\|^2), \
\ \ i, j = 1,2,\ldots,n, \ k \geq 0,
\end{align}
where
$$
\pi_{ij}^{(k)} = \frac{2p^2 - 3p + 1}{2p^2}
 - \frac{2p - 1}{p^2} \lambda_i \sum_{\ell
= 1}^{p - 1} \frac{1}{(d_i^{(k)})^{p - \ell} (d_j^{(k)})^\ell}  +
\frac{p - 1}{2p^2} \lambda^2_i \sum_{\ell = 1}^{2p - 1}
\frac{1}{(d_i^{(k)})^{2p - \ell} (d_j^{(k)})^\ell}.
$$
 Since $d_j^{(k)}$ converges to $\lambda_j^{1/p}$ as $k \to
\infty$ for each $j = 1, \ldots, n$, we may write $d_j^{(k)} =
\lambda_j^{1/p} + \varepsilon_j^{(k)}, j = 1,2,\ldots,n, \ k \geq
0$, where $\varepsilon_j^{(k)} \to 0$ as $k \to \infty$. Hence, for
each $i, j = 1,2,\ldots,n, \ k = 0,1,\ldots,$
\begin{equation}
\label{eq:pi_ij_k} \pi_{ij}^{(k)} = \frac{2p^2 - 3p + 1}{2p^2} -
\frac{2p - 1}{p^2} \sum_{j = 1}^{p - 1}
\left(\frac{\lambda_i}{\lambda_j}\right)^{j/p} + \frac{p - 1}{2p^2}
\sum_{j = 1}^{2p - 1} \left(\frac{\lambda_i}{\lambda_j}\right)^{j/p}
+ O(\varepsilon^{(k)}).
\end{equation}
where $\varepsilon^{(k)} := \max\limits_i|\varepsilon^{(k)}_i|$ and
the constant interfere with ``$O$" is independent on $i, j = 1, 2,
\ldots, n$.

To guarantee the numerical stability of Euler iteration
(\ref{it:EM}), we should require from (\ref{eq:wide_deltak+1}) and
(\ref{eq:pi_ij_k}) that
\begin{equation}
\label{neq:pi(k)neq1} \left|\frac{2p^2 - 3p + 1}{2p^2} - \frac{2p -
1}{p^2} \sum_{j = 1}^{p - 1}
\left(\frac{\lambda_i}{\lambda_j}\right)^{j/p} + \frac{p - 1}{2p^2}
\sum_{j = 1}^{2p - 1}
\left(\frac{\lambda_i}{\lambda_j}\right)^{j/p}\right| \leq 1, \ \ i,
j = 1,\ldots, n.
\end{equation}
Obviously, this is a very restrictive condition on $A$, even for
that $A$ is Hermitian positive definite. For example, in the case $p
= 2$, (\ref{neq:pi(k)neq1}) becomes
$$
-3 \leq - 5 \kappa_2(A)^{1/2} + \kappa_2(A) + \kappa_2(A)^{3/2} \leq
5,
$$
which is equivalent to $\kappa_2(A) \leq 5$, where $\kappa_2(A) :=
\|A\|_2 \|\inv{A}\|_2$.

The above defects of Euler iteration (\ref{it:EM}) should be
weakened. One way often used is to modify Euler iteration
(\ref{it:EM}) into the following coupled version by introducing the
auxiliary matrix $N_k (k\geq 0)$:
\begin{equation}
\label{it:CoupledEM} \left\{
\begin{array}{l}
\displaystyle X_{k + 1} = X_k \left(\frac{(2 p^2 - 3p + 1)\I + 2(2p
- 1)N_k - (p - 1)N_k^2}{2p^2}\right), \ \ X_0 =
\I, \\
\displaystyle N_{k + 1} = \left(\frac{(2 p^2 - 3p + 1)\I + 2(2p -
1)N_k - (p - 1)N_k^2}{2p^2}\right)^{- p} N_k, \ \ N_0 = A.
\end{array} \right.
\end{equation}
Clearly, $N_k = AX_k^{-p}$ and $\{X_k\}$ generated by
(\ref{it:CoupledEM}) is same as the sequence of Euler method
(\ref{it:EM}). We call it coupled Euler's iteration. When the
sequence $\{X_k\}$ generated by (\ref{it:CoupledEM}) converges to
$A^{1/p}$, $N_k$ converges to $\I$. The computational cost of
iteration (\ref{it:CoupledEM}) is $2(4 + \vartheta \log_2p)n^3$
flops per step for some $\vartheta \in [1,2]$ by means of the binary
powering technique \cite[Algorithm 11.2.2]{Golub1996}.


Note that, while we purely use coupled Euler iteration
(\ref{it:CoupledEM}) to compute $A^{1/p}$,  bad numerical results
will still appear. A simple example (see TEST 1 in Section 4)
illustrates this observation.


In order to avoid poor numerical results, we make Schur
decomposition before we begin to use coupled Euler iteration
(\ref{it:CoupledEM}), similar to Algorithms 3 and 4 in
\cite{Iannazzo2008}, based on the idea of Algorithm 3.3 in
\cite{GuoHigham2006}. New algorithm is given as follows.

\begin{algorithm}
\floatname{algorithm}{算法}
\caption{Schur-Euler algorithm using
(\ref{it:CoupledEM}) for computing $A^{1/p}$} \label{al:SE} Given $A
\in \CS^{n \times n}$ with no nonpositive real eigenvalues, an
integer $p = 2^{k_0}q$ with $k_0 \geq 0$ and $q$ odd. This algorithm
computes $A^{1/p}$ via a Schur decomposition and Euler iteration.
%\newcounter{newlist}
\begin{list}{\arabic{newlist}.}{\usecounter{newlist}
\setlength{\rightmargin}{0em}\setlength{\leftmargin}{1.2em}}
\item
Compute the Schur decomposition of $A = QRQ^*$;
\item
If $q = 1$, then $k_1 = k_0$; else choose the smallest $k_1 \geq
k_0$ such that for each eigenvalue $\lambda$ of $A$,
$\lambda^{1/2^{k_1}} \in \MCR_{\text{E}}$ defined in
(\ref{set:R_pra});
\item
Compute $B = R^{1/2^{k_1}}$ by taking the square root $k_1$ times;
if $q = 1$, then set $X = QB\tran{Q}$; else continue;
\item
Compute $C = B^{1/q}$ by using the coupled Euler iteration
(\ref{it:CoupledEM}) and set $X = Q C^{2^{k_1 - k_0}} \tran{Q}$.
\end{list}
\end{algorithm}


Recall that, one square root costs $n^3/3$ flops and the evaluation
of (\ref{it:CoupledEM}) costs $2(4 + \vartheta \log_2p)n^3$ flops,
where $\vartheta \in [1,2]$. Thus, $25n^3$ flops for the real Schur
decomposition plus $k_1$ times square root, $k_1 - k_0$ times matrix
multiplications and $4n^3$ flops to form $X$, the total
computational cost of Algorithm \ref{al:SE} is about
$$
\left(29 + \frac{k_1}{3} + 2 (k_1 - k_0) + 2k_2(4 + \vartheta
\log_2p)\right) n^3 \ \text{flops}, \ \ \vartheta \in [1,2],
$$
where we assume that $k_2$ iterations of (\ref{it:CoupledEM}) are
done exactly. Numerical experiments in Section 4 show that,
Algorithm \ref{al:SE} has a well numerical behavior. Moreover, in
most cases, Algorithm \ref{al:SE} has less computational time and
also can save some square roots in preprocessing.






\section{Numerical Experiments}



The goal of numerical experiments in this section is to illustrate
that Euler's method is as good as Newton method and Halley method
for computing the principal $p$th root of a matrix. To support this
claim, we now test Algorithm \ref{al:SE} and compare its
computational performance with existing three algorithms presented
in \cite{GuoHigham2006,Iannazzo2008} on computational error,
computational time and the number of iterations.

Upto 6 Tests are performed here. Test 1 is to show that it may
suffers from bad numerical results when we purely use coupled Euler
iteration (\ref{it:CoupledEM}) to compute $A^{1/p}$ even though the
matrix $A$ is simple. Tests 2-6 used by many authors
\cite{GuoHigham2006,Guo2010,Iannazzo2006,Iannazzo2008,HighamLin2011}
are to compare numerical behavior with other algorithms. The results
show that, in most cases, Algorithm \ref{al:SE} requires less
computational time and less steps of iterations. Meanwhile, it has
more numerical accuracy of the computed solution than others
algorithms.

Our numerical experiments were carried out in MATLAB 7.0 running on
a PC Intel Pentium P6200 of 2.13 GHz CPU. To measure of the quality
of a computed solution $X$, we use the relative residual $\rho_A(X)$
and relative error $\textup{err}(X)$ as follows:
\begin{equation}
\label{eq:RelResErr} \rho_A(X) = \frac{\|A - X^p\|}{\|X\| \|\sum_{j
= 0}^{p - 1} \tran{(X^{p - 1 - j})}\otimes X^j\|}, \ \ \
\textup{err}(X) = \frac{\|A - X^p\|}{\|A\|},
\end{equation}
where $\otimes$ denotes the Kronecker product and $\|\cdot\|$
denotes the Frobenius norm. Note that the relative residual
$\rho_A(X)$ (given in \cite{GuoHigham2006}) is more practically
useful definition of relative residual (e.g., for testing purposes)
than relative error and that the averaged CPU time computed by the
standard MATLAB function \textsf{cputime}. The averaged time was
computed by repeating 100 times for each test matrix. Moreover, we
use `iter' to stand for the number of the iterations.


For simplicity, in the following tests, we denote
\begin{enumerate}
\item
SE: Schur-Euler algorithm, Algorithm \ref{al:SE};
\item
PSN: parameter Schur-Newton algorithm from \cite[Algorithm
3.3]{GuoHigham2006};
\item
SN: Schur-Newton algorithm from \cite[Algorithm 3]{Iannazzo2008};
\item
SH: Schur-Halley algorithm from \cite[Algorithm 4]{Iannazzo2008}.
\end{enumerate}
The iterations in the above four algorithms are stopped when $\|N_k
- \I\| < \sqrt{n}u/2$, where $n$ is the size of $A$ and $u = 2^{-52}
\approx2.2204\me-16$.


TEST 1.  We first give a simple example to illustrate that it
usually suffers from bad numerical results when we purely use
coupled Euler iteration (\ref{it:CoupledEM}) to compute $A^{1/p}$.
Consider the following simple $3 \times 3$ matrix
$$
A = \left[\begin{array}{ccc} 1 & 1/2 & 0 \\
1/2 & 2 & 1/2 \\
0 & 1/2 & 3 \end{array}\right],
$$
and compute the $p$th root $A^{1/p}$ for $p = 2:15$. In Figure
\ref{fig:relerr_CEM}, we give the relative error $\textup{err}(X)$
for each $p$. As one can see, for $p = 2:6$ the coupled Euler
iteration (\ref{it:CoupledEM}) gives good relative error (Figure
\ref{fig:relerr_CEM} (a)), but the errors deteriorate as $p = 7:15$
(Figure \ref{fig:relerr_CEM} (b)). Algorithm \ref{al:SE} (taking
some preprocessing before using oupled Euler iteration
(\ref{it:CoupledEM})) shows good numerical stability (Figure
\ref{fig:relerr_CEM} (c)).

\begin{figure}[h]
\centering
\subfigure[]{\includegraphics[width=0.32\textwidth]{fig_relerr_coupledEM_1.eps}}
\subfigure[]{\includegraphics[width=0.32\textwidth]{fig_relerr_coupledEM_2.eps}}
\subfigure[]{\includegraphics[width=0.32\textwidth]{fig_relerr_coupledEM_Schur_3.eps}}
\caption{(a) and (b) show the relative error for the $p$th root
$A^{1/p}$ by using coupled Euler iteration (\ref{it:CoupledEM}); (c)
is the result of Algorithm \ref{al:SE}.}\label{fig:relerr_CEM}
\end{figure}

TEST 2. In this test, we compare the relative error on computing the
principal $p$th root of the matrices \cite{HighamLin2011} (depend on
variable $\varepsilon$)
\begin{equation}
\label{mat:Ae}
A(\varepsilon) = \left[\begin{array}{cc} 1 & 1 \\
0 & 1 + \varepsilon \end{array}\right]
\end{equation}
for $p = 12, 15, 30$. We choose $\varepsilon = 10^{-t}$ with 65
equally spaced values of $t \in [0,16]$. As is pointed out in
\cite{HighamLin2011}, $A(\varepsilon)$ approaches a defective matrix
as $t$ increases. The relative errors for the four algorithms are
shown in Figure \ref{fig:relerr_Ae}. The numerical results of
Algorithm \ref{al:SE} are very good. Note that, there exists 9
values of $t$ cannot be computed by using PSN while $t$ approaches
to 16. We also computed the relative residuals of these four
algorithms on matrix $A(\varepsilon)$, the results were broadly
similar to those shown in Figure \ref{fig:relerr_Ae}.

\begin{figure}[h]
\centering
\subfigure[$p=12$]{\includegraphics[width=0.32\textwidth]{fig_relerr_se_spn_sn_sh_p12.eps}}
\subfigure[$p=15$]{\includegraphics[width=0.32\textwidth]{fig_relerr_se_spn_sn_sh_p15.eps}}
\subfigure[$p=30$]{\includegraphics[width=0.32\textwidth]{fig_relerr_se_spn_sn_sh_p30.eps}}
\caption{The relative errors for the four algorithms on matrix
(\ref{mat:Ae}).}\label{fig:relerr_Ae}
\end{figure}


TEST 3. To compare the computational time of the existing three
algorithms with Algorithm \ref{al:SE}, we select the classical test
matrix (Frank matrix) from the MATLAB \textsf{gallery} function with
$15 \times 15$ matrix which no nonpositive real eigenvalues and
compute the principal $p$th root for $10 \leq p \leq 300$. Note
that, the Frank matrix is an upper Hessenbery with determinant 1.
This test matrix was also used in
\cite{GuoHigham2006,GrecoIannazzo2010,Iannazzo2006}. The eigenvalues
of a Frank matrix are positive and occur in reciprocal pairs, half
of which are ill-conditioned. The averaged CPU time for any value of
$p$ is shown in Fingure \ref{fig:cputime}. As one can see in Figure
\ref{fig:cputime}, the required CPU time of Algorithm \ref{al:SE} is
generally less than other algorithms, especially, PSN.

\begin{figure}[h]
\centering
\subfigure[]{\includegraphics[width=0.42\textwidth]{fig_cputime_psn.eps}}
\subfigure[]{\includegraphics[width=0.42\textwidth]{fig_cputime_sn.eps}} \\
\subfigure[]{\includegraphics[width=0.42\textwidth]{fig_cputime_sh.eps}}
\subfigure[]{\includegraphics[width=0.42\textwidth]{fig_cputime_se.eps}}
\caption{The CPU time (in seconds) required by using the four
Algorithms to compute the principal $p$th root of $15 \times 15$
Frank matrix for $p = 10:300$.}\label{fig:cputime}
\end{figure}


TEST 4. Consider the following two matrices which take from
\cite{Guo2010} and \cite{Iannazzo2008}, respectively.
$$
S_1 = \left[\begin{array}{rrrr} 0.44 & - 0.88 & - 0.38 & - 0.50 \\
0.68 & 2.15 & 0.48 & 0.11 \\
0.61 & 0.77 & 2.14 & 1.04 \\
- 0.16 & - 0.30 & - 0.67 & 1.33
\end{array}\right], \ \ \
S_2 = \left[\begin{array}{ccc} - 1 & - 2 & 2 \\
- 4 & - 6 & 6 \\
- 4 & - 16 & 13 \end{array}\right].
$$
Let $A_1 = S_1^5$ and $A_2 = S_2^{15}$. The eigenvalues of $A_1$ are
$15.2477, 0.2724 \pm 16.0066 \,\textup{i}, 1.1030$ and of $A_2$ are
$1, 2, 3$. We now compute $A_1^{1/5}$ and $A_2^{1/15}$ using the
four algorithms. The computational results are given in Table
\ref{tab:test3}. The relative error is computed by $\|X - S\|/\|S\|$
and the relative residuals is computed by using
(\ref{eq:RelResErr}). As is shown in Table \ref{tab:test3},
Algorithm \ref{al:SE} has a less pronounced advantage for computing
$A_1^{1/5}$ and $A_2^{1/15}$.

\begin{table}[h]
\begin{center}
\begin{tabular}{cc@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c}
\hline
& Algorithm & CPU time (s)& iter & $k_1$ & $\rho(X)$ & err($X$) \\
\hline
$A_1^{1/5}$ \\
& PSN & 3.91e-03 & 5 & 2 & 1.75e-15 & 2.05e-15 \\
& SN & 3.28e-03 & 6 & 2 & 4.67e-16 & 1.05e-15 \\
& SH & 3.13e-03 & 3 & 2 & 6.15e-16 & 9.63e-16 \\
& SE & 3.59e-03 & 3 & 3 & 8.77e-16 & 1.55e-15 \\
$A_2^{1/15}$ \\
& PSN & 6.09e-03 & 5 & 5 & 1.48e-15 & 2.67e-08 \\
& SN & 5.17e-03 & 6 & 4 & 5.74e-15 & 2.67e-08 \\
& SH & 5.00e-03 & 3 & 4 & 7.82e-15 & 2.67e-08 \\
& SE & 4.53e-03 & 5 & 5 & 2.25e-14 & 2.67e-08 \\

\hline
\end{tabular}
\end{center}
\caption{Results for computing $A_1^{1/5}$ and $A_2^{1/15}$ by using
the four Algorithms. } \label{tab:test3}
\end{table}

TEST 5. Consider a random nonnormal $8 \times 8$ matrix constructed
as $A = Q R \tran{Q}$, where $Q$ is a random orthogonal matrix and
$R$ is in the Schur form with eigenvalues $\alpha_j \pm \mathrm{i}
\beta_j, \alpha_j = - j^2/10, \beta_j = - j, j = 1:n/2$ and elements
$(2j, 2j + 1)$ equal to $- 450$. This example was used in
\cite{GuoHigham2006} and \cite{Iannazzo2008} to compare the behavior
of Algorithms PSN, SN and SH. We use the four algorithms to compute
the $p$th of this random matrix and list in Table \ref{tab:test4}
the results in terms of CPU time, number of iterations, relative
residual and relative error. Algorithm \ref{al:SE} shows a good
accuracy and requires less computational time.

\begin{table}[h]
\begin{center}
\begin{tabular}{cc@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c}
\hline
& Algorithm & CPU time (s)& iter & $k_1$ & $\rho(X)$ & err($X$) \\
\hline
$p = 5$ \\
& PSN & 2.34e-03 & 5 & 3 & 8.68e-15 & 3.11e-12 \\
& SN & 2.97e-03 & 5 & 2 & 8.62e-15 & 3.08e-12 \\
& SH & 3.13e-03 & 3 & 2 & 8.54e-15 & 3.06e-12 \\
& SE & 1.41e-03 & 3 & 2 & 8.52e-15 & 3.05e-12 \\
$p = 7$ \\
& PSN & 5.00e-03 & 5 & 3 & 1.45e-14 & 3.83e-12 \\
& SN & 4.22e-03 & 4 & 2 & 1.50e-14 & 3.96e-12 \\
& SH & 4.38e-03 & 3 & 2 & 1.55e-14 & 4.07e-12 \\
& SE & 4.06e-03 & 4 & 2 & 1.47e-14 & 3.87e-12 \\
\hline
\end{tabular}
\end{center}
\caption{Results for a random nonnormal matrix by using the four
Algorithms. } \label{tab:test4}
\end{table}


TEST 6. We select the classical test matrices (prolate matrix and
Frank matrix) with no nonpositive real eigenvalues from the MATLAB
\textsf{gallery} function together with Hilbert matrix, and then
compute their principal $p$th root for $p = 18, 33, 81$ by using the
four algorithms. These test matrices also were used in
\cite{Iannazzo2006,GrecoIannazzo2010,GuoHigham2006}. Note that, the
prolate matrix is a symmetric ill-conditioned Toeplitz matrix whose
elements are $A_{ii} = 1/2, A_{ij} = \sin(\pi(j-i)/2)/(\pi(j - i))$.
The Hilbert matrix is a notable example of an ill-conditioned
matrix. The elements of the Hilbert matrix are $H_{ij} = 1/(i + j -
1)$. The results of the comparison are summarized in Table
\ref{tab:relres_err_it}. As we can see that, in most cases,
Algorithm \ref{al:SE} has less computational time and smaller errors
than others algorithms. It also confirms that Euler's method is a
very good choice for computing $p$th roots of a matrix.

\begin{landscape}
\begin{table}
\begin{center}
%\begin{tabular}{cc|c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c}
\begin{tabular}{cccccccccccccccccc}
\hline
& \multicolumn{4}{c}{$p = 18$} &&& \multicolumn{4}{c}{$p = 33$} &&& \multicolumn{4}{c}{$p = 81$}\\
\cline{2-6} \cline{8-12} \cline{14-18}
& CPU time & iter & $k_1$ & $\rho(X)$ & err($X$)&& CPU time & iter & $k_1$ & $\rho(X)$ & err($X$) && CPU time & iter & $k_1$ & $\rho(X)$ & err($X$)\\
\hline
 \\
 \multicolumn{6}{l}{Hilbert matrix ($7 \times 7$)} \\
 \\
PSN & 1.06e-02 & 5 & 5 & 1.31e-14 & 6.47e-14 & & 1.06e-02 & 5 & 5 & 3.77e-14 & 2.12e-13 && 1.05e-02 & 5 & 5 & 6.77e-14 & 4.30e-13\\
SN & 8.44e-03 & 6 & 4 & 6.09e-15 & 3.01e-14 & & 8.59e-03 & 6 & 4 & 1.65e-14 & 9.31e-14 && 8.59e-02 & 6 & 4 & 2.63e-14 & 1.67e-13\\
SH & 8.59e-03 & 4 & 4 & 1.34e-15 & 6.60e-15 & & 8.59e-03 & 4 & 4 & 2.43e-14 & 1.37e-13 && 8.59e-03 & 4 & 4 & 2.51e-14 & 1.60e-13\\
SE & 7.03e-03 & 5 & 3 & 3.19e-15 & 1.57e-14 & & 8.44e-03 & 4 & 4 & 4.53e-15 & 2.55e-14 && 8.44e-03 & 4 & 4 & 2.45e-14 & 1.56e-13\\
 \\
\multicolumn{6}{l}{Prolate matrix ($10 \times 10$)}\\
 \\
PSN & 2.16e-02 & 5 & 5 & 3.96e-15 & 3.45e-14 & & 2.03e-02 & 5 & 5 & 1.33e-14 & 1.03e-13 && 2.31e-02 & 4 & 5 & 3.79e-14 & 3.64e-13\\
SN & 1.81e-02 & 6 & 4 & 1.29e-15 & 1.12e-14 & & 1.81e-02 & 6 & 4 & 5.74e-15 & 5.26e-14 && 1.77e-02 & 6 & 4 & 9.05e-15 & 8.70e-14\\
SH & 1.72e-02 & 4 & 4 & 1.84e-15 & 1.61e-14 & & 1.59e-02 & 4 & 4 & 1.03e-14 & 9.47e-14 && 1.81e-02 & 4 & 4 & 2.55e-14 & 2.45e-13\\
SE & 1.23e-02 & 4 & 3 & 1.29e-15 & 1.13e-14 & & 1.33e-02 & 4 & 3 & 3.41e-15 & 3.13e-14 && 1.25e-02 & 4 & 3 & 1.08e-14 & 1.04e-13\\
 \\
\multicolumn{6}{l}{Frank matrix ($12 \times 12$)}\\
 \\
PSN & 2.31e-02 & 5 & 4 & 1.46e-15 & 1.54e-08 & & 2.39e-02 & 5 & 4 & 7.65e-15 & 2.51e-08 && 2.44e-02 & 5 & 4 & 1.17e-13 & 6.58e-08\\
SN & 1.69e-02 & 6 & 3 & 1.46e-15 & 1.55e-08 & & 1.67e-02 & 6 & 3 & 5.06e-15 & 1.66e-08 && 1.70e-02 & 6 & 3 & 1.15e-13 & 6.45e-08\\
SH & 1.77e-02 & 4 & 3 & 1.52e-15 & 1.61e-08 & & 1.67e-02 & 4 & 3 & 8.09e-15 & 2.65e-08 && 1.80e-02 & 4 & 3 & 8.24e-14 & 4.73e-08\\
SE & 1.63e-02 & 4 & 3 & 1.17e-15 & 1.24e-08 & & 1.80e-02 & 4 & 3 & 7.45e-15 & 2.44e-08 && 1.78e-02 & 4 & 3 & 1.07e-13 & 6.00e-08\\
\hline
\end{tabular}
\end{center}
\caption{Results for computing principal $p$th root of $7 \times 7$
Hilbert matrix, $10 \times 10$ Prolate matrix and $12 \times 12$
Frank matrix by using the four Algorithms. }
\label{tab:relres_err_it}
\end{table}
\end{landscape}












\section{关于~Halley~法的注记}

While the iterative form of Halley's method for computing $A^{1/p}$
is given by:
\begin{equation}
\label{it:HM} X_{k+1} = X_k\left[(p+1)X_k^p + (p-1)A\right]^{-1}
\left[(p-1)X_k^p + (p+1)A\right],\ \ \ k=0,1,2,\ldots,
\end{equation}
provided $X_0$ commutes with $A$ and that $(p+1)X_k^p + (p-1)A$ is
nonsingular for any $k \geq 0$.


令
\begin{equation}
\label{fun:phi(z)} \phi_\nu(z) := 1 - u^p(z)(1 - z), \ \ \  z \in
\MCD_{0,\nu}, \ \nu = 1,2,
\end{equation}
其中
\begin{equation*}
u_2(z) := \frac{1-\frac{p-1}{2p}z}{1 - \frac{p+1}{2p}z}, z \in
\MCD_{0,2},
\end{equation*}

\begin{equation}
\label{set:D0} \MCD_{0,2} := \left\{z \in \CS: |z| < \frac{2p}{p +
1}\right\}.
\end{equation}


Define
\begin{equation}
\label{set:R} \MCR_\nu := \left\{z\in \CS: 1 - z \in
\overline{\MCD}_1 \bigcup \left(\MCD_{0,\nu} \bigcap
\MCD_{2,\nu}\right)\right\}
\end{equation}
and
\begin{equation}
\label{set:R_hat} \widehat{\MCR}_\nu := \left\{z\in\CS: 1-z \in
\mathcal {D}_1 \bigcup \left(\mathcal {D}_{0,\nu} \bigcap \mathcal
{D}_{2,\nu}\right)\right\}, \ \ \ \nu = 1,2,
\end{equation}
where $\MCD_{0,\nu}$ are defined in (\ref{set:D0}),
$\overline{\MCD}_1$ is the closure of $\MCD_1$ defined by
\begin{equation}
\label{set:D1} \MCD_1  := \{z \in \CS: |z| < 1\},
\end{equation}
and
\begin{equation}
\label{set:D2} \left\{
\begin{array}{l}
\displaystyle \MCD_{2,1}  := \left\{z \in \CS: \sup_{m \geq 2}
\left\{\frac{|S_{1,m}(z)|}{|z|}\right\} \cdot \frac{|z|
+|\phi_1(z)|}{\big||z| - |\phi_1(z)|\big|} < 1 \right\},\\
\displaystyle \MCD_{2,2}  := \left\{z \in \CS: \sup_{m \geq 3}
\left\{\frac{|S_{2,m}(z)|}{|z|}\right\} \cdot \frac{|z|
+|\phi_2(z)|}{\big||z| - |\phi_2(z)|\big|} < 1 \right\},
\end{array}
\right.
\end{equation}
where
\begin{equation}
\label{ser:Sm(z)} \left\{
\begin{array}{ll}
\displaystyle S_{1,m}(z) = \sum\limits_{j = 2}^{m} c_{1,j} z^{j},
& z \in \MCD_{0,1},\\
\displaystyle S_{2,m}(z) = \sum\limits_{j = 3}^{m} c_{2,j} z^{j}, &
z \in \MCD_{0,2},
\end{array}
\right.
\end{equation}
$c_{\nu,j} = \phi^{(j)}_\nu(0)/j!$ and $\phi_\nu$ are defined in
(\ref{fun:phi(z)}), $\nu = 1,2$.  As for $\phi_2(z)$, the similar
result has been proved in \cite{Lin2010}.






Similar to Newton's method (\ref{it:NM}), we can obtain the
following theorem on convergence and convergence order for Halley's
method (\ref{it:HM}).

\begin{theorem}
\label{th:MatrixHMCon} If all eigenvalues of $A \in \CS^{n \times
n}$ are in $\MCR_2$ defined by $(\ref{set:R})$ and all zero
eigenvalues of $A$ are semisimple, then the matrix sequence
$\{X_k\}$ generated by Halley's method $(\ref{it:HM})$ starting from
$X_0 = \I$ converges to the principal $p$th root $A^{1/p}$.
Moreover, if all eigenvalues are in
$\widehat{\MCR}_2\backslash\{0\}$ defined by $(\ref{set:R_hat})$,
then the convergence is cubic.
\end{theorem}







\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p25m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p25m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p25m500.eps}}\\
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p100m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p100m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p100m500.eps}}\\
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p400m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p400m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p400m500.eps}}\\
\caption{The approximating regions of $\MCR_2$ defined in
(\ref{set:R}) for $p = 25, 100, 400$ and $m = 20, 100, 500$, where
the red and blue regions denote the sets $\{z\in\CS: 1-z \in
\overline{\MCD}_1\}$ and $\{z\in \CS: 1-z \in \MCD_{0,2} \bigcap
\MCD_{2,2}\}$, respectively. }\label{fig:HM_ConvReg}
\end{figure}


\begin{remark}
Similar to Newton's method, in Figure \ref{fig:HM_ConvReg}, we plot
the approximating region $\MCR_2$ defined in (\ref{set:R}) with nine
cases in the complex plane for Halley's method, where the red and
blue regions denote the sets $\{z \in \CS: 1-z \in
\overline{\MCD}_1\}$ and $\{z\in \CS: 1-z \in
\MCD_{0,2}\bigcap\MCD_{2,2}\}$, respectively. We can see from Figure
\ref{fig:HM_ConvReg} that, for a fixed $p$, approximating regions
(cf. (a)-(c) for $p=25$, (d)-(f) for $p=100$ and (g)-(i) for
$p=400$) are almost the same when $m=20, 100, 500$, respectively. To
this end, it suffices to choose $m=20$ in practical numerical
computation. Furthermore, we find that, for a fixed $m(\geq20)$, the
approximating regions $\MCR_2$ with various $p$ are also almost the
same.
\end{remark}





\begin{remark}
It is also inconvenient, similar to the case of $\MCR_1$ defined in
(\ref{set:R}) for Newton's method, to check whether a eigenvalue
$\lambda$ belongs to $\MCR_2$ defined in (\ref{set:R}) in practice.
We can also define a feasible region which is acceptable
approximation to $\MCR_2$ and allows us to determine easily whether
a eigenvalue belongs to it. Define
\begin{equation}
\label{set:R_H_pra} \MCR_2^{\text{H}} = \MCD_3 \bigcup \MCD_5,
\end{equation}
where $\MCD_3$ is defined in (\ref{set:D3}) and
\begin{equation*}
\MCD_5 := \left\{z\in\CS: |\arg(z)|<\frac{\pi}{3},
|1-z|<\frac{7}{5}\right\}.
\end{equation*}
The actual convergence region $\MCR_2$ and the new feasible region
$\MCR_2^{\text{H}}$ are depicted in Figure \ref{fig:HM_PraConvReg}
with $p=7, 20, 100$ and fixed $m=20$, where the yellow parts denote
the region $\MCR_2^{\text{H}}$.
\end{remark}

\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_PraConvReg_p7m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_PraConvReg_p20m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_PraConvReg_p100m20.eps}}\\
\caption{For fixed $m=20$ and $p=7,20,100$, the actual convergence
regions $\MCR_2$ defined in (\ref{set:R}) (the union of the red and
blue parts) and the approximate convergence regions
$\MCR_2^{\text{H}}$ defined in (\ref{set:R_H_pra}) (the yellow
parts).}\label{fig:HM_PraConvReg}
\end{figure}




\begin{remark}
It is worth noting that, the convergence regions $\MCR_2$ defined in
(\ref{set:R}) for Halley's method (\ref{it:HM}) is more useful than
the one given by Iannazzo in \cite[Algorithm 4]{Iannazzo2008} in
which the choice of the region (the disk of center 8/5 and radius 1)
is heuristic and is based on the observation the experimental
regions of convergence. The comparison of the regions $\MCR_2$,
$\MCR_2^{\text{H}}$ and the disk of center 8/5 and radius 1 is shown
in Figure \ref{fig:HM_PraConvRegVSIannazzo}.
\end{remark}


\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p100m20_Iannazzo.eps}}
\quad\quad\quad\quad
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_PraConvReg.eps}}\\
\caption{(a) The convergence region $\MCR_2$ defined in
(\ref{set:R}) with $p=100$ and $m=20$ (the union of the red and blue
parts) and the the disk of center 8/5 and radius 1 given by Iannazzo
(the yellow parts); (b) the feasible region $\MCR_2^{\text{H}}$
defined in (\ref{set:R_H_pra}) (the red part) and the disk of center
8/5 and radius 1 (the blue
part).}\label{fig:HM_PraConvRegVSIannazzo}
\end{figure}
